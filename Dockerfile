{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNqNPmpWMxFhRp/9bTx1bYZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7ccc86941dfb4d109b44dec9f862afaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4eb5c394645b43dd9047d7483244887a",
              "IPY_MODEL_52b4bd1779fd4b28b7e686a5883ee56e",
              "IPY_MODEL_e54b6fbe805e4ddc86ecd42fe2bd30f4"
            ],
            "layout": "IPY_MODEL_143aa43c20a748e7bde0d48e9ded7a0a"
          }
        },
        "4eb5c394645b43dd9047d7483244887a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4812dd0c80114a23a24185440ae9fc84",
            "placeholder": "​",
            "style": "IPY_MODEL_d780a0a82e934083bd0cf906b35e438c",
            "value": "Listing: "
          }
        },
        "52b4bd1779fd4b28b7e686a5883ee56e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87385ac427d44395a1b2e91509afa9fd",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a038a53d937749f5a7000333fa59cc8b",
            "value": 1
          }
        },
        "e54b6fbe805e4ddc86ecd42fe2bd30f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f65893d3976242ff91899db89dbc5277",
            "placeholder": "​",
            "style": "IPY_MODEL_0f216149366a4abf829b694687155e41",
            "value": " 33/? [12:57&lt;00:00,  5.43s/page]"
          }
        },
        "143aa43c20a748e7bde0d48e9ded7a0a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4812dd0c80114a23a24185440ae9fc84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d780a0a82e934083bd0cf906b35e438c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "87385ac427d44395a1b2e91509afa9fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "a038a53d937749f5a7000333fa59cc8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f65893d3976242ff91899db89dbc5277": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f216149366a4abf829b694687155e41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9dc2f7ad551843d4bb16b0ba50174701": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c32d4750dc6e41abb7b0575238a75b62",
              "IPY_MODEL_bd06b15ea863470ea2c787c3da52ed4c",
              "IPY_MODEL_7108ee24712248e69b230ac77f2a4d83"
            ],
            "layout": "IPY_MODEL_62ec7cb6cabf45ab8ceb1e18a09bcd06"
          }
        },
        "c32d4750dc6e41abb7b0575238a75b62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b3601136cf34461a28b1907b4e38671",
            "placeholder": "​",
            "style": "IPY_MODEL_321f0a2189334f06bd427d5d57ef4c5e",
            "value": "Listing pages: "
          }
        },
        "bd06b15ea863470ea2c787c3da52ed4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e2b5a3d431ea43439a73fe6725a7191d",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_be932cc4d52a4c988a39860659b6d980",
            "value": 0
          }
        },
        "7108ee24712248e69b230ac77f2a4d83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5b3ffe0e617425ab7eef897817e4b49",
            "placeholder": "​",
            "style": "IPY_MODEL_1943921901dd447ab47e7ef3c3b278f0",
            "value": " 0/? [00:02&lt;?, ?page/s]"
          }
        },
        "62ec7cb6cabf45ab8ceb1e18a09bcd06": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b3601136cf34461a28b1907b4e38671": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "321f0a2189334f06bd427d5d57ef4c5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e2b5a3d431ea43439a73fe6725a7191d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "be932cc4d52a4c988a39860659b6d980": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b5b3ffe0e617425ab7eef897817e4b49": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1943921901dd447ab47e7ef3c3b278f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9277f433937d43578b6dc14673e0f971": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9021ed39b91f480f8cb195c6b7724e64",
              "IPY_MODEL_55cc75f64c0f46e5bca31a345f120af2",
              "IPY_MODEL_4fb0d2bda2474a50a53bf77a8592bc57"
            ],
            "layout": "IPY_MODEL_50ac0a5207cd45f9945f22df1fd92e23"
          }
        },
        "9021ed39b91f480f8cb195c6b7724e64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9b3110c6acb49ab8206798ab647f55d",
            "placeholder": "​",
            "style": "IPY_MODEL_786461cca19840efa5f147bc41dd66d1",
            "value": "Listing pages: "
          }
        },
        "55cc75f64c0f46e5bca31a345f120af2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "info",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f6578b4221e44c9b20e7de121e5cdb5",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d4fe4a1063b743738d4ac1346874bb15",
            "value": 1
          }
        },
        "4fb0d2bda2474a50a53bf77a8592bc57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7cb7b78d80564367b9fdd91e13b88c06",
            "placeholder": "​",
            "style": "IPY_MODEL_9148514e5b85481b88bbf42537392348",
            "value": " 227/? [18:15&lt;00:00,  4.35s/page]"
          }
        },
        "50ac0a5207cd45f9945f22df1fd92e23": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9b3110c6acb49ab8206798ab647f55d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "786461cca19840efa5f147bc41dd66d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7f6578b4221e44c9b20e7de121e5cdb5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "d4fe4a1063b743738d4ac1346874bb15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7cb7b78d80564367b9fdd91e13b88c06": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9148514e5b85481b88bbf42537392348": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HamzaElHammoutidev/boursenews/blob/main/Dockerfile\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install requests beautifulsoup4 lxml html5lib pandas tqdm tenacity python-dateutil\n"
      ],
      "metadata": {
        "id": "C-eAPncSyOag"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ny3-LmHpyGEt",
        "outputId": "8806ca5f-4b40-4151-93d2-2a4368f48215"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[PAGE] === 1 ===\n",
            "[GET] https://boursenews.ma/articles/marches\n",
            "[GET] https://boursenews.ma/article/marches/feuille-de-marche\n",
            "[ARTICLE] Marchés\n",
            "          https://boursenews.ma/article/marches/feuille-de-marche\n",
            "          Total so far: 1\n",
            "[GET] https://boursenews.ma/article/marches/capex-akdital-croissance\n",
            "[ARTICLE] Marchés\n",
            "          https://boursenews.ma/article/marches/capex-akdital-croissance\n",
            "          Total so far: 2\n",
            "[GET] https://boursenews.ma/article/marches/Akdital-t2-2025\n",
            "[ARTICLE] Marchés\n",
            "          https://boursenews.ma/article/marches/Akdital-t2-2025\n",
            "          Total so far: 3\n",
            "[GET] https://boursenews.ma/article/marches/akdital-3-nouveaux-etablissements-oriental\n",
            "[ARTICLE] Marchés\n",
            "          https://boursenews.ma/article/marches/akdital-3-nouveaux-etablissements-oriental\n",
            "          Total so far: 4\n",
            "[GET] https://boursenews.ma/article/marches/delta-holding-adduction-barrage-moulay-abdellah\n",
            "[ARTICLE] Marchés\n",
            "          https://boursenews.ma/article/marches/delta-holding-adduction-barrage-moulay-abdellah\n",
            "          Total so far: 5\n",
            "[GET] https://boursenews.ma/article/marches/Cmgp-s1-2025\n",
            "[ARTICLE] Marchés\n",
            "          https://boursenews.ma/article/marches/Cmgp-s1-2025\n",
            "          Total so far: 6\n",
            "[GET] https://boursenews.ma/article/marches/jet-contractors-pluie-de-nouveaux-contrats\n",
            "[ARTICLE] Marchés\n",
            "          https://boursenews.ma/article/marches/jet-contractors-pluie-de-nouveaux-contrats\n",
            "          Total so far: 7\n",
            "[GET] https://boursenews.ma/article/marches/deltaholding-chiffre-affaires-S1-2025\n",
            "[ARTICLE] Marchés\n",
            "          https://boursenews.ma/article/marches/deltaholding-chiffre-affaires-S1-2025\n",
            "          Total so far: 8\n",
            "[GET] https://boursenews.ma/article/marches/Jet-marche-onda-marrakech\n",
            "[ARTICLE] Marchés\n",
            "          https://boursenews.ma/article/marches/Jet-marche-onda-marrakech\n",
            "          Total so far: 9\n",
            "[GET] https://boursenews.ma/article/marches/masi-analyse-technique-aout-2025\n",
            "[ARTICLE] Marchés\n",
            "          https://boursenews.ma/article/marches/masi-analyse-technique-aout-2025\n",
            "          Total so far: 10\n",
            "\n",
            "[PAGE] === 2 ===\n",
            "[GET] https://boursenews.ma/articles/marches/2\n",
            "[GET] https://boursenews.ma/article/marches/Inflation-maroc-juillet-2025\n",
            "[ARTICLE] Marchés\n",
            "          https://boursenews.ma/article/marches/Inflation-maroc-juillet-2025\n",
            "          Total so far: 11\n",
            "[GET] https://boursenews.ma/article/marches/Trump-prevoit-de-taxer-l-acier-et-les-semi-conducteurs-dans-les-prochaines-semaines\n",
            "[ARTICLE] Marchés\n",
            "          https://boursenews.ma/article/marches/Trump-prevoit-de-taxer-l-acier-et-les-semi-conducteurs-dans-les-prochaines-semaines\n",
            "          Total so far: 12\n",
            "[GET] https://boursenews.ma/article/marches/Les-Bourses-europeennes-evoluent-en-hausse-vendredi\n",
            "[ARTICLE] Marchés\n",
            "          https://boursenews.ma/article/marches/Les-Bourses-europeennes-evoluent-en-hausse-vendredi\n",
            "          Total so far: 13\n",
            "[GET] https://boursenews.ma/article/marches/Hebdo-OPCVM-au-08-08-2025\n",
            "[ARTICLE] Marchés\n",
            "          https://boursenews.ma/article/marches/Hebdo-OPCVM-au-08-08-2025\n",
            "          Total so far: 14\n",
            "[GET] https://boursenews.ma/article/marches/le-Nikkei-franchit-les-43348,50-points-un-nouveau-record\n",
            "[ARTICLE] Marchés\n",
            "          https://boursenews.ma/article/marches/le-Nikkei-franchit-les-43348,50-points-un-nouveau-record\n",
            "          Total so far: 15\n",
            "[GET] https://boursenews.ma/article/marches/SMI-indicateurs-S1-2025\n",
            "[ARTICLE] Marchés\n",
            "          https://boursenews.ma/article/marches/SMI-indicateurs-S1-2025\n",
            "          Total so far: 16\n",
            "[GET] https://boursenews.ma/article/marches/maroc-deficit-budgetaire-juillet-2025\n",
            "[ARTICLE] Marchés\n",
            "          https://boursenews.ma/article/marches/maroc-deficit-budgetaire-juillet-2025\n",
            "          Total so far: 17\n",
            "[GET] https://boursenews.ma/article/marches/BKGR-recommande-d-accumuler-IAM-avec-un-potentiel-de-hausse-de-13,7%25\n",
            "[ARTICLE] Marchés\n",
            "          https://boursenews.ma/article/marches/BKGR-recommande-d-accumuler-IAM-avec-un-potentiel-de-hausse-de-13,7%25\n",
            "          Total so far: 18\n",
            "[GET] https://boursenews.ma/article/marches/Managem-indicateurs-S1-2025\n",
            "[ARTICLE] Marchés\n",
            "          https://boursenews.ma/article/marches/Managem-indicateurs-S1-2025\n",
            "          Total so far: 19\n",
            "[GET] https://boursenews.ma/article/marches/badr-alioua-signe-son-grand-retour-dans-la-gestion-dactifs\n",
            "[ARTICLE] Marchés\n",
            "          https://boursenews.ma/article/marches/badr-alioua-signe-son-grand-retour-dans-la-gestion-dactifs\n",
            "          Total so far: 20\n",
            "\n",
            "[PAGE] === 3 ===\n",
            "[GET] https://boursenews.ma/articles/marches/3\n",
            "[GET] https://boursenews.ma/article/marches/Trump-prolonge-de-90-jours-la-treve-commerciale-avec-la-Chine\n",
            "[ARTICLE] Marchés\n",
            "          https://boursenews.ma/article/marches/Trump-prolonge-de-90-jours-la-treve-commerciale-avec-la-Chine\n",
            "          Total so far: 21\n",
            "[GET] https://boursenews.ma/article/marches/AMMC-Programme-de-rachat-juillet-2025\n",
            "[ARTICLE] Marchés\n",
            "          https://boursenews.ma/article/marches/AMMC-Programme-de-rachat-juillet-2025\n",
            "          Total so far: 22\n",
            "[GET] https://boursenews.ma/article/marches/Enquete-trimestrielle-de-Bank-Al-Maghrib-sur-les-taux-debiteurs-T2-2025\n",
            "[ARTICLE] Marchés\n",
            "          https://boursenews.ma/article/marches/Enquete-trimestrielle-de-Bank-Al-Maghrib-sur-les-taux-debiteurs-T2-2025\n",
            "          Total so far: 23\n",
            "[GET] https://boursenews.ma/article/marches/Bourses-europeennes-ouvrent-dans-le-vert-ce-lundi\n",
            "[ARTICLE] Marchés\n",
            "          https://boursenews.ma/article/marches/Bourses-europeennes-ouvrent-dans-le-vert-ce-lundi\n",
            "          Total so far: 24\n",
            "[GET] https://boursenews.ma/article/marches/LabelVie-Saham-Capital-Gestion-maintient-sa-recommandation-acheter\n",
            "[ARTICLE] Marchés\n",
            "          https://boursenews.ma/article/marches/LabelVie-Saham-Capital-Gestion-maintient-sa-recommandation-acheter\n",
            "          Total so far: 25\n",
            "[GET] https://boursenews.ma/article/marches/Bourse-de-Casablanca-qui-profitera-du-mega-proje-hydraulique-du-Gharb\n",
            "[ARTICLE] Marchés\n",
            "          https://boursenews.ma/article/marches/Bourse-de-Casablanca-qui-profitera-du-mega-proje-hydraulique-du-Gharb\n",
            "          Total so far: 26\n",
            "[GET] https://boursenews.ma/article/marches/CMT-compte-rebours-lance\n",
            "[ARTICLE] Marchés\n",
            "          https://boursenews.ma/article/marches/CMT-compte-rebours-lance\n",
            "          Total so far: 27\n",
            "[GET] https://boursenews.ma/article/marches/Patron-TGCC-recu-en-audience-pa-le-president-du-Gabon\n",
            "[ARTICLE] Marchés\n",
            "          https://boursenews.ma/article/marches/Patron-TGCC-recu-en-audience-pa-le-president-du-Gabon\n",
            "          Total so far: 28\n",
            "[GET] https://boursenews.ma/article/marches/plf-2026-les-priorites-gouvernement\n",
            "[ARTICLE] Marchés\n",
            "          https://boursenews.ma/article/marches/plf-2026-les-priorites-gouvernement\n",
            "          Total so far: 29\n",
            "[GET] https://boursenews.ma/article/marches/BKGR-marche-obligataire-la-detente-se-poursuit-sur-le-court-terme\n",
            "[ARTICLE] Marchés\n",
            "          https://boursenews.ma/article/marches/BKGR-marche-obligataire-la-detente-se-poursuit-sur-le-court-terme\n",
            "          Total so far: 30\n",
            "\n",
            "[PAGE] === 4 ===\n",
            "[GET] https://boursenews.ma/articles/marches/4\n",
            "[GET] https://boursenews.ma/article/marches/BKGR-banques-deficit-de-liquidite-du-31-juillet-au-06-aout\n",
            "[ARTICLE] Marchés\n",
            "          https://boursenews.ma/article/marches/BKGR-banques-deficit-de-liquidite-du-31-juillet-au-06-aout\n",
            "          Total so far: 31\n",
            "[GET] https://boursenews.ma/article/marches/Tourisme-arrivees-touristiques-a-fin-juillet-2025\n",
            "[ARTICLE] Marchés\n",
            "          https://boursenews.ma/article/marches/Tourisme-arrivees-touristiques-a-fin-juillet-2025\n",
            "          Total so far: 32\n",
            "[GET] https://boursenews.ma/article/marches/labelvie-indicateurs-s1-2025\n",
            "[ARTICLE] Marchés\n",
            "          https://boursenews.ma/article/marches/labelvie-indicateurs-s1-2025\n",
            "          Total so far: 33\n",
            "[GET] https://boursenews.ma/article/marches/Placements-dans-quoi-investissent-les-assureurs\n",
            "[ARTICLE] Marchés\n",
            "          https://boursenews.ma/article/marches/Placements-dans-quoi-investissent-les-assureurs\n",
            "          Total so far: 34\n",
            "[GET] https://boursenews.ma/article/marches/Hebdo-OPCVM-au-01-08-2025\n",
            "[ARTICLE] Marchés\n",
            "          https://boursenews.ma/article/marches/Hebdo-OPCVM-au-01-08-2025\n",
            "          Total so far: 35\n",
            "[GET] https://boursenews.ma/article/marches/cih-bank-cloture-augmentation-de-capital\n",
            "[ARTICLE] Marchés\n",
            "          https://boursenews.ma/article/marches/cih-bank-cloture-augmentation-de-capital\n",
            "          Total so far: 36\n",
            "[GET] https://boursenews.ma/article/marches/portrait-robot-de-l-emprunteur-marocain\n",
            "[ARTICLE] Marchés\n",
            "          https://boursenews.ma/article/marches/portrait-robot-de-l-emprunteur-marocain\n",
            "          Total so far: 37\n",
            "[GET] https://boursenews.ma/article/marches/Trump-menace-d-augmenter-les-droits-de-douane-sur-l-UE\n",
            "[ARTICLE] Marchés\n",
            "          https://boursenews.ma/article/marches/Trump-menace-d-augmenter-les-droits-de-douane-sur-l-UE\n",
            "          Total so far: 38\n",
            "[GET] https://boursenews.ma/article/marches/Red-Med-Corporate-Marsa-Maroc-Raja-Club-Athletic\n",
            "[ARTICLE] Marchés\n",
            "          https://boursenews.ma/article/marches/Red-Med-Corporate-Marsa-Maroc-Raja-Club-Athletic\n",
            "          Total so far: 39\n",
            "[GET] https://boursenews.ma/article/marches/ad-capital-asset-management-certification-isae-3402-type-1\n",
            "[ARTICLE] Marchés\n",
            "          https://boursenews.ma/article/marches/ad-capital-asset-management-certification-isae-3402-type-1\n",
            "          Total so far: 40\n",
            "\n",
            "[PAGE] === 5 ===\n",
            "[GET] https://boursenews.ma/articles/marches/5\n",
            "[GET] https://boursenews.ma/article/marches/Ciments-ventes-juillet-2025\n",
            "[ARTICLE] Marchés\n",
            "          https://boursenews.ma/article/marches/Ciments-ventes-juillet-2025\n",
            "          Total so far: 41\n",
            "[GET] https://boursenews.ma/article/marches/tresor-besoin-previsionnel-aout-2025\n",
            "[ARTICLE] Marchés\n",
            "          https://boursenews.ma/article/marches/tresor-besoin-previsionnel-aout-2025\n",
            "          Total so far: 42\n",
            "[GET] https://boursenews.ma/article/marches/gestion-collective-actif-opcvm-detenu-par-16-investissseurs\n",
            "[ARTICLE] Marchés\n",
            "          https://boursenews.ma/article/marches/gestion-collective-actif-opcvm-detenu-par-16-investissseurs\n",
            "          Total so far: 43\n",
            "[GET] https://boursenews.ma/article/marches/automobile-forte-croissance-ventes-automobile-auto-hall\n",
            "[ARTICLE] Marchés\n",
            "          https://boursenews.ma/article/marches/automobile-forte-croissance-ventes-automobile-auto-hall\n",
            "          Total so far: 44\n",
            "[GET] https://boursenews.ma/article/marches/Maghreb-Titrisation-et-CDG-Capital-lancent-la-premiere-titrisation-verte-de-creances-commerciales-au-Maroc\n",
            "[ARTICLE] Marchés\n",
            "          https://boursenews.ma/article/marches/Maghreb-Titrisation-et-CDG-Capital-lancent-la-premiere-titrisation-verte-de-creances-commerciales-au-Maroc\n",
            "          Total so far: 45\n",
            "[GET] https://boursenews.ma/article/marches/societes-de-bourses-benefices-explosent-2024\n",
            "[ARTICLE] Marchés\n",
            "          https://boursenews.ma/article/marches/societes-de-bourses-benefices-explosent-2024\n",
            "          Total so far: 46\n",
            "[GET] https://boursenews.ma/article/marches/marche-auto-les-ventes-en-forte-hausse-en-juillet-2025\n",
            "[ARTICLE] Marchés\n",
            "          https://boursenews.ma/article/marches/marche-auto-les-ventes-en-forte-hausse-en-juillet-2025\n",
            "          Total so far: 47\n",
            "[GET] https://boursenews.ma/article/marches/petrole-WTI-chute-a-66,5-dollars-le-baril-apres-la-hausse-des-prix-de-l-OPEP\n",
            "[ARTICLE] Marchés\n",
            "          https://boursenews.ma/article/marches/petrole-WTI-chute-a-66,5-dollars-le-baril-apres-la-hausse-des-prix-de-l-OPEP\n",
            "          Total so far: 48\n",
            "[GET] https://boursenews.ma/article/marches/Les-Bourses-europeennes-ouvrent-en-hausse-ce-lundi\n",
            "[ARTICLE] Marchés\n",
            "          https://boursenews.ma/article/marches/Les-Bourses-europeennes-ouvrent-en-hausse-ce-lundi\n",
            "          Total so far: 49\n",
            "[GET] https://boursenews.ma/article/marches/Disway-indicateurs-financiers-au-S1-2025\n",
            "[ARTICLE] Marchés\n",
            "          https://boursenews.ma/article/marches/Disway-indicateurs-financiers-au-S1-2025\n",
            "          Total so far: 50\n",
            "\n",
            "[INFO] Saved 50 articles.\n",
            "[INFO] CSV  : /content/boursenews_marches/boursenews_marches.csv\n",
            "[INFO] JSONL: /content/boursenews_marches/boursenews_marches.jsonl\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# Boursenews \"Marchés\" scraper with logs (Colab-ready)\n",
        "# =========================\n",
        "\n",
        "\n",
        "import re, time, random, json, os, sys\n",
        "from urllib.parse import urljoin\n",
        "from datetime import datetime\n",
        "from dateutil import parser as dateparser\n",
        "from typing import List, Dict, Optional\n",
        "\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "BASE = \"https://boursenews.ma\"\n",
        "SECTION = \"/articles/marches\"\n",
        "START_URL = urljoin(BASE, SECTION)\n",
        "\n",
        "# -------------------------\n",
        "# Config\n",
        "# -------------------------\n",
        "OUTPUT_DIR = \"/content/boursenews_marches\"\n",
        "MAX_PAGES: Optional[int] = 5       # for testing, set None for all\n",
        "REQUEST_DELAY = (1.0, 2.0)\n",
        "TIMEOUT = 20\n",
        "HEADERS = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) \"\n",
        "                  \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "                  \"Chrome/124.0 Safari/537.36\"\n",
        "}\n",
        "KEEP_ON_AFTER_DATE: Optional[str] = None\n",
        "KEEP_ON_BEFORE_DATE: Optional[str] = None\n",
        "\n",
        "# -------------------------\n",
        "# HTTP + Helpers\n",
        "# -------------------------\n",
        "class HttpError(Exception): pass\n",
        "\n",
        "session = requests.Session()\n",
        "session.headers.update(HEADERS)\n",
        "\n",
        "@retry(\n",
        "    reraise=True,\n",
        "    retry=retry_if_exception_type((requests.RequestException, HttpError)),\n",
        "    wait=wait_exponential(multiplier=1, min=1, max=30),\n",
        "    stop=stop_after_attempt(5),\n",
        ")\n",
        "def get(url: str) -> requests.Response:\n",
        "    print(f\"[GET] {url}\")\n",
        "    resp = session.get(url, timeout=TIMEOUT, allow_redirects=True)\n",
        "    if resp.status_code >= 500:\n",
        "        raise HttpError(f\"Server error {resp.status_code}\")\n",
        "    if resp.status_code == 429:\n",
        "        retry_after = int(resp.headers.get(\"Retry-After\", \"5\"))\n",
        "        print(f\"[WARN] 429 Too Many Requests, sleeping {retry_after}s\")\n",
        "        time.sleep(retry_after)\n",
        "        raise HttpError(\"Rate limited\")\n",
        "    resp.raise_for_status()\n",
        "    return resp\n",
        "\n",
        "def sleep_a_bit():\n",
        "    d = random.uniform(*REQUEST_DELAY)\n",
        "    time.sleep(d)\n",
        "\n",
        "def clean_whitespace(s: str) -> str:\n",
        "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
        "\n",
        "# -------------------------\n",
        "# Parse functions\n",
        "# -------------------------\n",
        "def parse_listing(html: str) -> List[str]:\n",
        "    soup = BeautifulSoup(html, \"lxml\")\n",
        "    links = []\n",
        "    for h3 in soup.select(\"h3 a[href]\"):\n",
        "        links.append(urljoin(BASE, h3[\"href\"]))\n",
        "    return list(dict.fromkeys(links))\n",
        "\n",
        "def parse_article(html: str, url: str) -> Dict:\n",
        "    soup = BeautifulSoup(html, \"lxml\")\n",
        "    title = soup.select_one(\"h1\")\n",
        "    title = clean_whitespace(title.get_text(\" \", strip=True)) if title else None\n",
        "    content = soup.select_one(\"article\") or soup\n",
        "    paragraphs = [clean_whitespace(p.get_text(\" \", strip=True)) for p in content.find_all(\"p\")]\n",
        "    text = \"\\n\\n\".join([p for p in paragraphs if p])\n",
        "    return {\n",
        "        \"url\": url,\n",
        "        \"title\": title,\n",
        "        \"content_text\": text[:500] + \"...\" if text else None\n",
        "    }\n",
        "\n",
        "def date_in_range(date_iso: Optional[str]) -> bool:\n",
        "    if not (KEEP_ON_AFTER_DATE or KEEP_ON_BEFORE_DATE):\n",
        "        return True\n",
        "    try:\n",
        "        d = datetime.fromisoformat(date_iso).date()\n",
        "        if KEEP_ON_AFTER_DATE and d < datetime.fromisoformat(KEEP_ON_AFTER_DATE).date():\n",
        "            return False\n",
        "        if KEEP_ON_BEFORE_DATE and d > datetime.fromisoformat(KEEP_ON_BEFORE_DATE).date():\n",
        "            return False\n",
        "        return True\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "# -------------------------\n",
        "# Crawl\n",
        "# -------------------------\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "items = []\n",
        "seen = set()\n",
        "\n",
        "def listing_url(page: int) -> str:\n",
        "    return START_URL if page == 1 else f\"{START_URL}/{page}\"\n",
        "\n",
        "def crawl():\n",
        "    page = 1\n",
        "    while True:\n",
        "        if MAX_PAGES and page > MAX_PAGES:\n",
        "            break\n",
        "        print(f\"\\n[PAGE] === {page} ===\")\n",
        "        try:\n",
        "            resp = get(listing_url(page))\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] Failed listing page {page}: {e}\")\n",
        "            break\n",
        "        links = parse_listing(resp.text)\n",
        "        if not links:\n",
        "            print(\"[INFO] No more links, stopping.\")\n",
        "            break\n",
        "        for link in links:\n",
        "            if link in seen:\n",
        "                continue\n",
        "            seen.add(link)\n",
        "            try:\n",
        "                aresp = get(link)\n",
        "            except Exception as e:\n",
        "                print(f\"[ERROR] Failed article {link}: {e}\")\n",
        "                continue\n",
        "            art = parse_article(aresp.text, link)\n",
        "            items.append(art)\n",
        "            print(f\"[ARTICLE] {art['title']}\")\n",
        "            print(f\"          {link}\")\n",
        "            print(f\"          Total so far: {len(items)}\")\n",
        "            sleep_a_bit()\n",
        "        page += 1\n",
        "\n",
        "crawl()\n",
        "\n",
        "# -------------------------\n",
        "# Save\n",
        "# -------------------------\n",
        "df = pd.DataFrame(items)\n",
        "csv_path = os.path.join(OUTPUT_DIR, \"boursenews_marches.csv\")\n",
        "jsonl_path = os.path.join(OUTPUT_DIR, \"boursenews_marches.jsonl\")\n",
        "\n",
        "df.to_csv(csv_path, index=False)\n",
        "with open(jsonl_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    for row in items:\n",
        "        f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(f\"\\n[INFO] Saved {len(items)} articles.\")\n",
        "print(f\"[INFO] CSV  : {csv_path}\")\n",
        "print(f\"[INFO] JSONL: {jsonl_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# High-speed Boursenews \"Marchés\" scraper (Colab-ready, async + HTTP/2 + selectolax)\n",
        "# - Crawls https://boursenews.ma/articles/marches[/<page>]\n",
        "# - Async httpx with HTTP/2, connection pooling, bounded concurrency, lightweight retries\n",
        "# - Fast parsing via selectolax (lexbor engine)\n",
        "# - Progress logs + periodic checkpoints (CSV + JSONL)\n",
        "# - Optional date filters to short-circuit deep crawls\n",
        "\n",
        "!pip -q install httpx[http2] selectolax pandas tqdm python-dateutil orjson\n",
        "\n",
        "import asyncio, time, re, math, os, json, random, orjson\n",
        "from datetime import datetime, date\n",
        "from urllib.parse import urljoin\n",
        "from typing import Optional, List, Dict, Set\n",
        "import httpx\n",
        "from selectolax.parser import HTMLParser\n",
        "import pandas as pd\n",
        "from dateutil import parser as dateparser\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "BASE = \"https://boursenews.ma\"\n",
        "SECTION = \"/articles/marches\"\n",
        "START_URL = urljoin(BASE, SECTION)\n",
        "\n",
        "# --------------------\n",
        "# Config (adjust freely)\n",
        "# --------------------\n",
        "OUTPUT_DIR = \"/content/boursenews_marches_fast\"\n",
        "MAX_PAGES: Optional[int] = None        # e.g., 40; None = auto until empty\n",
        "CONCURRENCY = 20                       # parallel article fetches; 20–64 is usually fine\n",
        "REQS_PER_SEC = 6                       # gentle rate limiter (overall)\n",
        "TIMEOUT = 20.0\n",
        "CHECKPOINT_EVERY = 50                  # save every N articles\n",
        "KEEP_ON_AFTER_DATE: Optional[str] = None  # \"2025-07-01\"\n",
        "KEEP_ON_BEFORE_DATE: Optional[str] = None  # \"2025-12-31\"\n",
        "\n",
        "HEADERS = {\n",
        "    \"user-agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 \"\n",
        "                  \"KHTML, like Gecko) Chrome/124.0 Safari/537.36\"\n",
        "}\n",
        "\n",
        "# --------------------\n",
        "# Utils\n",
        "# --------------------\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "CSV_PATH = os.path.join(OUTPUT_DIR, \"boursenews_marches.csv\")\n",
        "JSONL_PATH = os.path.join(OUTPUT_DIR, \"boursenews_marches.jsonl\")\n",
        "\n",
        "def clean_ws(s: str) -> str:\n",
        "    return re.sub(r\"\\s+\", \" \", s).strip() if s else s\n",
        "\n",
        "def parse_date_iso(text: Optional[str]) -> Optional[str]:\n",
        "    if not text:\n",
        "        return None\n",
        "    try:\n",
        "        dt = dateparser.parse(text, dayfirst=True, fuzzy=True, languages=[\"fr\", \"en\"])\n",
        "        return dt.date().isoformat() if dt else None\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def date_in_range(date_iso: Optional[str]) -> bool:\n",
        "    if not (KEEP_ON_AFTER_DATE or KEEP_ON_BEFORE_DATE):\n",
        "        return True\n",
        "    if not date_iso:\n",
        "        return False\n",
        "    try:\n",
        "        d = datetime.fromisoformat(date_iso).date()\n",
        "        if KEEP_ON_AFTER_DATE and d < datetime.fromisoformat(KEEP_ON_AFTER_DATE).date():\n",
        "            return False\n",
        "        if KEEP_ON_BEFORE_DATE and d > datetime.fromisoformat(KEEP_ON_BEFORE_DATE).date():\n",
        "            return False\n",
        "        return True\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def listing_url(page: int) -> str:\n",
        "    return START_URL if page == 1 else f\"{START_URL}/{page}\"\n",
        "\n",
        "# -------------\n",
        "# Rate limiter\n",
        "# -------------\n",
        "# Simple token bucket per process; good enough for one Colab runtime\n",
        "class RateLimiter:\n",
        "    def __init__(self, rps: float):\n",
        "        self.interval = 1.0 / max(0.01, rps)\n",
        "        self._last = 0.0\n",
        "        self._lock = asyncio.Lock()\n",
        "    async def wait(self):\n",
        "        async with self._lock:\n",
        "            now = time.perf_counter()\n",
        "            delta = self._last + self.interval - now\n",
        "            if delta > 0:\n",
        "                await asyncio.sleep(delta)\n",
        "                self._last = time.perf_counter()\n",
        "            else:\n",
        "                self._last = now\n",
        "\n",
        "rate_limiter = RateLimiter(REQS_PER_SEC)\n",
        "\n",
        "# --------------------\n",
        "# Checkpointing (resume-safe)\n",
        "# --------------------\n",
        "def load_seen_urls() -> Set[str]:\n",
        "    seen = set()\n",
        "    if os.path.exists(JSONL_PATH):\n",
        "        with open(JSONL_PATH, \"rb\") as f:\n",
        "            for line in f:\n",
        "                try:\n",
        "                    obj = orjson.loads(line)\n",
        "                    if \"url\" in obj:\n",
        "                        seen.add(obj[\"url\"])\n",
        "                except Exception:\n",
        "                    pass\n",
        "    return seen\n",
        "\n",
        "def append_jsonl(objs: List[Dict]):\n",
        "    with open(JSONL_PATH, \"ab\") as f:\n",
        "        for o in objs:\n",
        "            f.write(orjson.dumps(o, option=orjson.OPT_APPEND_NEWLINE))\n",
        "\n",
        "def write_csv_all():\n",
        "    # Convert entire JSONL to CSV (idempotent)\n",
        "    rows = []\n",
        "    if os.path.exists(JSONL_PATH):\n",
        "        with open(JSONL_PATH, \"rb\") as f:\n",
        "            for line in f:\n",
        "                try:\n",
        "                    rows.append(orjson.loads(line))\n",
        "                except Exception:\n",
        "                    pass\n",
        "    pd.DataFrame(rows).to_csv(CSV_PATH, index=False)\n",
        "\n",
        "# --------------------\n",
        "# Networking with retries\n",
        "# --------------------\n",
        "async def fetch(client: httpx.AsyncClient, url: str, *, max_attempts=5) -> Optional[httpx.Response]:\n",
        "    backoff = 1.0\n",
        "    for attempt in range(1, max_attempts + 1):\n",
        "        await rate_limiter.wait()\n",
        "        try:\n",
        "            r = await client.get(url, timeout=TIMEOUT)\n",
        "            # Handle rate limit or 5xx\n",
        "            if r.status_code == 429:\n",
        "                retry_after = int(r.headers.get(\"retry-after\", \"3\"))\n",
        "                print(f\"[429] {url} – sleeping {retry_after}s\")\n",
        "                await asyncio.sleep(retry_after)\n",
        "                raise httpx.HTTPStatusError(\"429\", request=r.request, response=r)\n",
        "            if r.status_code >= 500:\n",
        "                raise httpx.HTTPStatusError(str(r.status_code), request=r.request, response=r)\n",
        "            r.raise_for_status()\n",
        "            return r\n",
        "        except Exception as e:\n",
        "            if attempt == max_attempts:\n",
        "                print(f\"[ERROR] {url} – failed after {attempt} attempts: {e}\")\n",
        "                return None\n",
        "            sleep_for = backoff + random.uniform(0, 0.5)\n",
        "            print(f\"[WARN] {url} – attempt {attempt} failed ({e}); retrying in {sleep_for:.1f}s\")\n",
        "            await asyncio.sleep(sleep_for)\n",
        "            backoff = min(backoff * 2, 30.0)\n",
        "\n",
        "# --------------------\n",
        "# Parsers (selectolax)\n",
        "# --------------------\n",
        "def parse_listing(html: str) -> List[str]:\n",
        "    tree = HTMLParser(html)\n",
        "    urls = set()\n",
        "    # typical: <h3><a href=\"/article/...\">...</a></h3>\n",
        "    for a in tree.css(\"h3 a[href]\"):\n",
        "        href = a.attributes.get(\"href\", \"\").strip()\n",
        "        if href:\n",
        "            urls.add(urljoin(BASE, href))\n",
        "    # fallback: any /article/ link\n",
        "    for a in tree.css(\"a[href]\"):\n",
        "        href = a.attributes.get(\"href\", \"\").strip()\n",
        "        if \"/article\" in href:\n",
        "            urls.add(urljoin(BASE, href))\n",
        "    return list(urls)\n",
        "\n",
        "def parse_article(html: str, url: str) -> Dict:\n",
        "    tree = HTMLParser(html)\n",
        "    # Title\n",
        "    title = None\n",
        "    for sel in (\"h1.article-title\", \"h1\", \".post-title h1\"):\n",
        "        node = tree.css_first(sel)\n",
        "        if node and node.text():\n",
        "            title = clean_ws(node.text())\n",
        "            break\n",
        "    if not title:\n",
        "        if tree.head and tree.head.css_first(\"title\"):\n",
        "            title = clean_ws(tree.head.css_first(\"title\").text() or \"\")\n",
        "\n",
        "    # meta area (date/author often near title)\n",
        "    date_text = None\n",
        "    author = None\n",
        "    for sel in (\"h2\", \"h3\", \".post-meta\", \".entry-meta\", \".article-meta\", \".meta\"):\n",
        "        for n in tree.css(sel):\n",
        "            txt = clean_ws(n.text() or \"\")\n",
        "            if re.search(r\"\\d{4}\", txt):\n",
        "                date_text = txt\n",
        "                m = re.search(r\"\\s+-\\s*par\\s+(.*)$\", txt, flags=re.I)\n",
        "                if m:\n",
        "                    author = m.group(1).strip()\n",
        "                break\n",
        "        if date_text:\n",
        "            break\n",
        "\n",
        "    date_iso = parse_date_iso(date_text)\n",
        "\n",
        "    # content (pick the first container that has <p>)\n",
        "    content = None\n",
        "    for sel in (\"article .content\", \".post-content\", \".entry-content\",\n",
        "                \".article-content\", \"article\", \".article\", \"#content\"):\n",
        "        node = tree.css_first(sel)\n",
        "        if node and node.css_first(\"p\"):\n",
        "            content = node\n",
        "            break\n",
        "    if not content:\n",
        "        content = tree.body or tree\n",
        "\n",
        "    paragraphs = []\n",
        "    if content:\n",
        "        for p in content.css(\"p, li\"):\n",
        "            t = clean_ws(p.text() or \"\")\n",
        "            if len(t) > 2:\n",
        "                paragraphs.append(t)\n",
        "    content_text = \"\\n\\n\".join(paragraphs) if paragraphs else None\n",
        "\n",
        "    # images\n",
        "    images = []\n",
        "    for img in content.css(\"img[src]\"):\n",
        "        src = (img.attributes.get(\"src\") or \"\").strip()\n",
        "        if src:\n",
        "            images.append(urljoin(url, src))\n",
        "    if images:\n",
        "        # dedupe, preserve order\n",
        "        seen = set(); tmp = []\n",
        "        for i in images:\n",
        "            if i not in seen:\n",
        "                seen.add(i); tmp.append(i)\n",
        "        images = tmp\n",
        "\n",
        "    return {\n",
        "        \"url\": url,\n",
        "        \"title\": title,\n",
        "        \"date_text\": date_text,\n",
        "        \"date_iso\": date_iso,\n",
        "        \"author\": author,\n",
        "        \"images\": images or None,\n",
        "        \"content_text\": content_text\n",
        "    }\n",
        "\n",
        "# --------------------\n",
        "# Crawl\n",
        "# --------------------\n",
        "async def crawl():\n",
        "    limits = httpx.Limits(max_connections=64, max_keepalive_connections=32, keepalive_expiry=30.0)\n",
        "    async with httpx.AsyncClient(http2=True, headers=HEADERS, base_url=BASE, limits=limits, follow_redirects=True) as client:\n",
        "        seen_urls = load_seen_urls()\n",
        "        total_saved = len(seen_urls)\n",
        "        print(f\"[INFO] Resuming with {total_saved} articles already saved.\")\n",
        "\n",
        "        page = 1\n",
        "        pbar = tqdm(unit=\"page\", desc=\"Listing\")\n",
        "        while True:\n",
        "            if MAX_PAGES and page > MAX_PAGES:\n",
        "                break\n",
        "            url = listing_url(page)\n",
        "            resp = await fetch(client, url)\n",
        "            if not resp:\n",
        "                print(f\"[ERROR] listing page {page} failed, stopping loop.\")\n",
        "                break\n",
        "            article_urls = parse_listing(resp.text)\n",
        "            article_urls = [u for u in article_urls if u not in seen_urls]\n",
        "            if not article_urls:\n",
        "                # if two consecutive empties, stop; here we keep it simple: single empty -> stop\n",
        "                print(\"[INFO] No new links on this page; assuming end.\")\n",
        "                break\n",
        "\n",
        "            print(f\"[PAGE {page}] {len(article_urls)} new links\")\n",
        "            sem = asyncio.Semaphore(CONCURRENCY)\n",
        "\n",
        "            batch_results: List[Dict] = []\n",
        "\n",
        "            async def fetch_one(aurl: str):\n",
        "                async with sem:\n",
        "                    r = await fetch(client, aurl)\n",
        "                    if not r:\n",
        "                        return None\n",
        "                    parsed = parse_article(r.text, aurl)\n",
        "                    # optional filter\n",
        "                    if not date_in_range(parsed.get(\"date_iso\")):\n",
        "                        return None\n",
        "                    return parsed\n",
        "\n",
        "            tasks = [asyncio.create_task(fetch_one(u)) for u in article_urls]\n",
        "            for coro in asyncio.as_completed(tasks):\n",
        "                item = await coro\n",
        "                if item:\n",
        "                    append_jsonl([item])  # append immediately (crash-safe)\n",
        "                    seen_urls.add(item[\"url\"])\n",
        "                    total_saved += 1\n",
        "                    if total_saved % CHECKPOINT_EVERY == 0:\n",
        "                        # refresh CSV every N to keep a view\n",
        "                        write_csv_all()\n",
        "                    if total_saved % 10 == 0:\n",
        "                        print(f\"[PROGRESS] Saved: {total_saved}\")\n",
        "\n",
        "            # continue to next page\n",
        "            page += 1\n",
        "            pbar.update(1)\n",
        "\n",
        "        pbar.close()\n",
        "        # Final CSV write\n",
        "        write_csv_all()\n",
        "        print(f\"[DONE] Total saved: {total_saved}\")\n",
        "        print(f\"[PATH] JSONL: {JSONL_PATH}\")\n",
        "        print(f\"[PATH] CSV  : {CSV_PATH}\")\n",
        "\n",
        "await crawl()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "7ccc86941dfb4d109b44dec9f862afaf",
            "4eb5c394645b43dd9047d7483244887a",
            "52b4bd1779fd4b28b7e686a5883ee56e",
            "e54b6fbe805e4ddc86ecd42fe2bd30f4",
            "143aa43c20a748e7bde0d48e9ded7a0a",
            "4812dd0c80114a23a24185440ae9fc84",
            "d780a0a82e934083bd0cf906b35e438c",
            "87385ac427d44395a1b2e91509afa9fd",
            "a038a53d937749f5a7000333fa59cc8b",
            "f65893d3976242ff91899db89dbc5277",
            "0f216149366a4abf829b694687155e41"
          ]
        },
        "id": "0kvvLZn0zQN6",
        "outputId": "5e5d2294-8942-47c1-a4df-e9e23ca533ed"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/5.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/5.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m\r\u001b[2K   \u001b[91m━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.7/5.8 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m4.1/5.8 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h[INFO] Resuming with 0 articles already saved.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Listing: 0page [00:00, ?page/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7ccc86941dfb4d109b44dec9f862afaf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PAGE 1] 34 new links\n",
            "[PROGRESS] Saved: 10\n",
            "[PROGRESS] Saved: 20\n",
            "[PROGRESS] Saved: 30\n",
            "[PAGE 2] 11 new links\n",
            "[PROGRESS] Saved: 40\n",
            "[PAGE 3] 11 new links\n",
            "[PROGRESS] Saved: 50\n",
            "[PAGE 4] 11 new links\n",
            "[PROGRESS] Saved: 60\n",
            "[PAGE 5] 11 new links\n",
            "[PROGRESS] Saved: 70\n",
            "[PAGE 6] 11 new links\n",
            "[PROGRESS] Saved: 80\n",
            "[PAGE 7] 11 new links\n",
            "[PROGRESS] Saved: 90\n",
            "[PROGRESS] Saved: 100\n",
            "[PAGE 8] 11 new links\n",
            "[PROGRESS] Saved: 110\n",
            "[PAGE 9] 11 new links\n",
            "[PROGRESS] Saved: 120\n",
            "[PAGE 10] 11 new links\n",
            "[PROGRESS] Saved: 130\n",
            "[PAGE 11] 11 new links\n",
            "[PROGRESS] Saved: 140\n",
            "[PAGE 12] 11 new links\n",
            "[PROGRESS] Saved: 150\n",
            "[PAGE 13] 11 new links\n",
            "[PROGRESS] Saved: 160\n",
            "[PAGE 14] 11 new links\n",
            "[PROGRESS] Saved: 170\n",
            "[PAGE 15] 11 new links\n",
            "[PROGRESS] Saved: 180\n",
            "[PAGE 16] 11 new links\n",
            "[PROGRESS] Saved: 190\n",
            "[PAGE 17] 11 new links\n",
            "[PROGRESS] Saved: 200\n",
            "[PROGRESS] Saved: 210\n",
            "[PAGE 18] 11 new links\n",
            "[PROGRESS] Saved: 220\n",
            "[PAGE 19] 11 new links\n",
            "[PROGRESS] Saved: 230\n",
            "[PAGE 20] 11 new links\n",
            "[PROGRESS] Saved: 240\n",
            "[PAGE 21] 11 new links\n",
            "[PROGRESS] Saved: 250\n",
            "[PAGE 22] 11 new links\n",
            "[PROGRESS] Saved: 260\n",
            "[PAGE 23] 11 new links\n",
            "[PROGRESS] Saved: 270\n",
            "[PAGE 24] 11 new links\n",
            "[PROGRESS] Saved: 280\n",
            "[PAGE 25] 11 new links\n",
            "[PROGRESS] Saved: 290\n",
            "[PAGE 26] 11 new links\n",
            "[PROGRESS] Saved: 300\n",
            "[PAGE 27] 11 new links\n",
            "[PROGRESS] Saved: 310\n",
            "[PROGRESS] Saved: 320\n",
            "[PAGE 28] 11 new links\n",
            "[PROGRESS] Saved: 330\n",
            "[PAGE 29] 11 new links\n",
            "[PROGRESS] Saved: 340\n",
            "[PAGE 30] 11 new links\n",
            "[PROGRESS] Saved: 350\n",
            "[PAGE 31] 11 new links\n",
            "[PROGRESS] Saved: 360\n",
            "[PAGE 32] 11 new links\n",
            "[PROGRESS] Saved: 370\n",
            "[PAGE 33] 11 new links\n",
            "[PROGRESS] Saved: 380\n",
            "[PAGE 34] 11 new links\n",
            "[PROGRESS] Saved: 390\n",
            "[WARN] https://boursenews.ma/article/marches/marche-obligataire-detente-prolonge-confort-budgetaire – attempt 1 failed (); retrying in 1.3s\n",
            "[WARN] https://boursenews.ma/article/marches/Abdellatif-Zaghnoun-marche-des-capitaux – attempt 1 failed (); retrying in 1.1s\n",
            "[WARN] https://boursenews.ma/article/marches/S2m-affectation-resultats – attempt 1 failed (); retrying in 1.4s\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "CancelledError",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1209375118.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[PATH] CSV  : {CSV_PATH}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m \u001b[0;32mawait\u001b[0m \u001b[0mcrawl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1209375118.py\u001b[0m in \u001b[0;36mcrawl\u001b[0;34m()\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0mtasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mu\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marticle_urls\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcoro\u001b[0m \u001b[0;32min\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_completed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                     \u001b[0mappend_jsonl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# append immediately (crash-safe)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/asyncio/tasks.py\u001b[0m in \u001b[0;36m_wait_for_one\u001b[0;34m()\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_wait_for_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m             \u001b[0;31m# Dummy value from _on_timeout().\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/asyncio/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m                 \u001b[0;32mawait\u001b[0m \u001b[0mgetter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m                 \u001b[0mgetter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Just in case getter is not done yet.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mCancelledError\u001b[0m: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[WARN] https://boursenews.ma/article/marches/TGCC-augmentation-capital – attempt 1 failed (); retrying in 1.0s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Boursenews \"Marchés\" – FULL CONTENT scraper\n",
        "# Colab-ready: async httpx (HTTP/2) + selectolax, streaming writers, low RAM\n",
        "# =========================\n",
        "\n",
        "!pip -q install httpx[http2] selectolax pandas python-dateutil orjson tqdm\n",
        "\n",
        "import asyncio, time, re, os, csv, gzip, orjson, json, random\n",
        "from urllib.parse import urljoin, urlparse\n",
        "from datetime import datetime\n",
        "from typing import Optional, List, Dict, Set\n",
        "import httpx\n",
        "from selectolax.parser import HTMLParser\n",
        "from dateutil import parser as dateparser\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "BASE = \"https://boursenews.ma\"\n",
        "SECTION = \"/articles/marches\"\n",
        "START_URL = f\"{BASE}{SECTION}\"\n",
        "\n",
        "# --------------------\n",
        "# Config (tune as needed)\n",
        "# --------------------\n",
        "OUTPUT_DIR = \"/content/boursenews_marches_full\"\n",
        "CONCURRENCY = 24            # 24–48 is fast & polite\n",
        "REQS_PER_SEC = 8            # overall rate budget\n",
        "TIMEOUT = 20.0\n",
        "MAX_PAGES: Optional[int] = None   # e.g., 40; None = keep going until pages look empty\n",
        "KEEP_ON_AFTER_DATE: Optional[str] = None  # e.g., \"2025-07-01\"\n",
        "KEEP_ON_BEFORE_DATE: Optional[str] = None # e.g., \"2025-12-31\"\n",
        "CHECKPOINT_EVERY = 50       # print extra progress every N items\n",
        "DOWNLOAD_IMAGES = False     # set True to also fetch images into /images\n",
        "\n",
        "HEADERS = {\n",
        "    \"user-agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 \"\n",
        "                  \"KHTML, like Gecko) Chrome/124.0 Safari/537.36\"\n",
        "}\n",
        "\n",
        "# --------------------\n",
        "# Output paths\n",
        "# --------------------\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "PAGES_DIR = os.path.join(OUTPUT_DIR, \"pages\")\n",
        "IMAGES_DIR = os.path.join(OUTPUT_DIR, \"images\")\n",
        "os.makedirs(PAGES_DIR, exist_ok=True)\n",
        "if DOWNLOAD_IMAGES:\n",
        "    os.makedirs(IMAGES_DIR, exist_ok=True)\n",
        "\n",
        "JSONL_PATH = os.path.join(OUTPUT_DIR, \"boursenews_marches.jsonl.gz\")\n",
        "CSV_PATH   = os.path.join(OUTPUT_DIR, \"boursenews_marches.csv\")\n",
        "SEEN_PATH  = os.path.join(OUTPUT_DIR, \"seen_urls.txt\")\n",
        "\n",
        "# --------------------\n",
        "# Utilities\n",
        "# --------------------\n",
        "def clean_ws(s: Optional[str]) -> Optional[str]:\n",
        "    if not s: return s\n",
        "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
        "\n",
        "def parse_date_iso(text: Optional[str]) -> Optional[str]:\n",
        "    if not text: return None\n",
        "    try:\n",
        "        dt = dateparser.parse(text, dayfirst=True, fuzzy=True, languages=[\"fr\",\"en\"])\n",
        "        return dt.date().isoformat() if dt else None\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def date_in_range(date_iso: Optional[str]) -> bool:\n",
        "    if not (KEEP_ON_AFTER_DATE or KEEP_ON_BEFORE_DATE):\n",
        "        return True\n",
        "    if not date_iso:\n",
        "        return False\n",
        "    try:\n",
        "        d = datetime.fromisoformat(date_iso).date()\n",
        "        if KEEP_ON_AFTER_DATE and d < datetime.fromisoformat(KEEP_ON_AFTER_DATE).date():\n",
        "            return False\n",
        "        if KEEP_ON_BEFORE_DATE and d > datetime.fromisoformat(KEEP_ON_BEFORE_DATE).date():\n",
        "            return False\n",
        "        return True\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def listing_url(page: int) -> str:\n",
        "    return START_URL if page == 1 else f\"{START_URL}/{page}\"\n",
        "\n",
        "def slugify(url: str) -> str:\n",
        "    path = urlparse(url).path.strip(\"/\").replace(\"/\", \"-\")\n",
        "    path = re.sub(r\"[^a-zA-Z0-9\\-_.]\", \"-\", path)[:120]\n",
        "    return path or \"article\"\n",
        "\n",
        "def absolutize_attrs(node, base_url: str, attr: str):\n",
        "    for el in node.css(f\"[{attr}]\"):\n",
        "        val = (el.attributes.get(attr) or \"\").strip()\n",
        "        if val:\n",
        "            el.attributes[attr] = urljoin(base_url, val)\n",
        "\n",
        "# --- REPLACE sanitize_article_container() WITH THIS ---\n",
        "def sanitize_article_container(tree: HTMLParser, base_url: str):\n",
        "    # choose container (no comma selectors)\n",
        "    candidates = [\n",
        "        \"article .content\", \".post-content\", \".entry-content\",\n",
        "        \".article-content\", \"article\", \".article\", \"#content\"\n",
        "    ]\n",
        "    container = None\n",
        "    for sel in candidates:\n",
        "        node = tree.css_first(sel)\n",
        "        if node and node.css_first(\"p\"):\n",
        "            container = node\n",
        "            break\n",
        "    if not container:\n",
        "        container = tree.body or tree\n",
        "\n",
        "    # remove junk (no \"a, b, c\" selectors)\n",
        "    junk_list = [\"script\", \"style\", \"nav\", \"aside\", \"iframe\", \"form\", \"noscript\"]\n",
        "    for j in junk_list:\n",
        "        for n in container.css(j):\n",
        "            n.decompose()\n",
        "\n",
        "    # absolutize href/src\n",
        "    for el in container.css(\"[href]\"):\n",
        "        val = (el.attributes.get(\"href\") or \"\").strip()\n",
        "        if val:\n",
        "            el.attributes[\"href\"] = urljoin(base_url, val)\n",
        "    for el in container.css(\"[src]\"):\n",
        "        val = (el.attributes.get(\"src\") or \"\").strip()\n",
        "        if val:\n",
        "            el.attributes[\"src\"] = urljoin(base_url, val)\n",
        "\n",
        "    # text from <p> and <li> separately\n",
        "    paragraphs = []\n",
        "    for tag in (\"p\", \"li\"):\n",
        "        for n in container.css(tag):\n",
        "            txt = clean_ws(n.text() or \"\")\n",
        "            if txt and len(txt) > 2:\n",
        "                paragraphs.append(txt)\n",
        "    content_text = \"\\n\\n\".join(paragraphs) if paragraphs else None\n",
        "\n",
        "    content_html_clean = container.html or None\n",
        "    return content_html_clean, content_text\n",
        "\n",
        "\n",
        "# --- REPLACE parse_article() WITH THIS (more defensive) ---\n",
        "def parse_article(html: str, url: str) -> Dict:\n",
        "    try:\n",
        "        tree = HTMLParser(html)\n",
        "    except Exception:\n",
        "        # if parse fails, keep raw bits minimal\n",
        "        return {\"url\": url, \"title\": None, \"date_text\": None,\n",
        "                \"date_iso\": None, \"author\": None, \"images\": None,\n",
        "                \"content_text\": None, \"content_html_clean\": None}\n",
        "\n",
        "    # Title (no comma selectors)\n",
        "    title = None\n",
        "    for sel in (\"h1.article-title\", \"h1\", \".post-title h1\"):\n",
        "        n = tree.css_first(sel)\n",
        "        if n and n.text():\n",
        "            title = clean_ws(n.text())\n",
        "            break\n",
        "    if not title and tree.head:\n",
        "        tnode = tree.head.css_first(\"title\")\n",
        "        if tnode and tnode.text():\n",
        "            title = clean_ws(tnode.text())\n",
        "\n",
        "    # Meta (date_text, author)\n",
        "    date_text, author = None, None\n",
        "    for sel in (\"h2\", \"h3\", \".post-meta\", \".entry-meta\", \".article-meta\", \".meta\"):\n",
        "        for node in tree.css(sel):\n",
        "            txt = clean_ws(node.text() or \"\")\n",
        "            if re.search(r\"\\d{4}\", txt):\n",
        "                date_text = txt\n",
        "                m = re.search(r\"\\s+-\\s*par\\s+(.*)$\", txt, flags=re.I)\n",
        "                if m:\n",
        "                    author = m.group(1).strip()\n",
        "                break\n",
        "        if date_text:\n",
        "            break\n",
        "    date_iso = parse_date_iso(date_text)\n",
        "\n",
        "    # Content\n",
        "    try:\n",
        "        content_html_clean, content_text = sanitize_article_container(tree, url)\n",
        "    except Exception:\n",
        "        content_html_clean, content_text = None, None\n",
        "\n",
        "    # Images from cleaned HTML\n",
        "    images = []\n",
        "    if content_html_clean:\n",
        "        try:\n",
        "            t2 = HTMLParser(content_html_clean)\n",
        "            for img in t2.css(\"img[src]\"):\n",
        "                src = (img.attributes.get(\"src\") or \"\").strip()\n",
        "                if src:\n",
        "                    images.append(src)\n",
        "            # dedupe\n",
        "            seen = set(); imgs2 = []\n",
        "            for s in images:\n",
        "                if s not in seen:\n",
        "                    seen.add(s); imgs2.append(s)\n",
        "            images = imgs2\n",
        "        except Exception:\n",
        "            images = None\n",
        "    else:\n",
        "        images = None\n",
        "\n",
        "    return {\n",
        "        \"url\": url,\n",
        "        \"title\": title,\n",
        "        \"date_text\": date_text,\n",
        "        \"date_iso\": date_iso,\n",
        "        \"author\": author,\n",
        "        \"images\": images,\n",
        "        \"content_text\": content_text,\n",
        "        \"content_html_clean\": content_html_clean\n",
        "    }\n",
        "\n",
        "\n",
        "    content_html_clean, content_text = sanitize_article_container(tree, url)\n",
        "\n",
        "    # capture images (absolute)\n",
        "    images = []\n",
        "    if content_html_clean:\n",
        "        t2 = HTMLParser(content_html_clean)\n",
        "        for img in t2.css(\"img[src]\"):\n",
        "            src = (img.attributes.get(\"src\") or \"\").strip()\n",
        "            if src:\n",
        "                images.append(src)\n",
        "        # dedupe keep order\n",
        "        seen = set(); imgs2=[]\n",
        "        for s in images:\n",
        "            if s not in seen:\n",
        "                seen.add(s); imgs2.append(s)\n",
        "        images = imgs2\n",
        "    else:\n",
        "        images = None\n",
        "\n",
        "    return {\n",
        "        \"url\": url,\n",
        "        \"title\": title,\n",
        "        \"date_text\": date_text,\n",
        "        \"date_iso\": date_iso,\n",
        "        \"author\": author,\n",
        "        \"images\": images,\n",
        "        \"content_text\": content_text,\n",
        "        \"content_html_clean\": content_html_clean\n",
        "    }\n",
        "\n",
        "# --------------------\n",
        "# Streaming writers (no big RAM)\n",
        "# --------------------\n",
        "def load_seen_urls() -> Set[str]:\n",
        "    if not os.path.exists(SEEN_PATH):\n",
        "        return set()\n",
        "    with open(SEEN_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "        return set(line.strip() for line in f if line.strip())\n",
        "\n",
        "def add_seen_url(u: str):\n",
        "    with open(SEEN_PATH, \"a\", encoding=\"utf-8\") as f:\n",
        "        f.write(u + \"\\n\")\n",
        "\n",
        "def append_jsonl_gz(obj: dict):\n",
        "    with gzip.open(JSONL_PATH, \"ab\") as f:\n",
        "        f.write(orjson.dumps(obj))\n",
        "        f.write(b\"\\n\")\n",
        "\n",
        "def append_csv_row(row: dict, fields=(\"url\",\"title\",\"date_text\",\"date_iso\",\"author\",\"snapshot_path\",\"images_count\")):\n",
        "    header_needed = not os.path.exists(CSV_PATH)\n",
        "    data = {\n",
        "        \"url\": row.get(\"url\"),\n",
        "        \"title\": row.get(\"title\"),\n",
        "        \"date_text\": row.get(\"date_text\"),\n",
        "        \"date_iso\": row.get(\"date_iso\"),\n",
        "        \"author\": row.get(\"author\"),\n",
        "        \"snapshot_path\": row.get(\"snapshot_path\"),\n",
        "        \"images_count\": len(row.get(\"images\") or []) if isinstance(row.get(\"images\"), list) else 0\n",
        "    }\n",
        "    with open(CSV_PATH, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        w = csv.DictWriter(f, fieldnames=list(fields))\n",
        "        if header_needed:\n",
        "            w.writeheader()\n",
        "        w.writerow(data)\n",
        "\n",
        "def save_snapshot_html(article: dict):\n",
        "    \"\"\"\n",
        "    Save a minimal standalone HTML file containing the cleaned content.\n",
        "    Returns relative path stored in CSV/JSONL.\n",
        "    \"\"\"\n",
        "    slug = slugify(article[\"url\"])\n",
        "    fname = f\"{slug}.html\"\n",
        "    fpath = os.path.join(PAGES_DIR, fname)\n",
        "    title = article.get(\"title\") or slug\n",
        "    body = article.get(\"content_html_clean\") or \"\"\n",
        "    # Tiny boilerplate\n",
        "    html_doc = f\"\"\"<!doctype html>\n",
        "<html lang=\"fr\"><head><meta charset=\"utf-8\">\n",
        "<title>{title}</title>\n",
        "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n",
        "<style>body{{font-family:system-ui,-apple-system,Segoe UI,Roboto;max-width:820px;margin:40px auto;padding:0 16px;line-height:1.6}}</style>\n",
        "</head><body>\n",
        "<h1>{title}</h1>\n",
        "<p><em>{article.get(\"date_text\") or \"\"}</em></p>\n",
        "<hr/>\n",
        "{body}\n",
        "</body></html>\"\"\"\n",
        "    with open(fpath, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(html_doc)\n",
        "    return os.path.relpath(fpath, OUTPUT_DIR)\n",
        "\n",
        "# --------------------\n",
        "# Simple rate limiter\n",
        "# --------------------\n",
        "class RateLimiter:\n",
        "    def __init__(self, rps: float):\n",
        "        self.interval = 1.0 / max(0.01, rps)\n",
        "        self._last = 0.0\n",
        "        self._lock = asyncio.Lock()\n",
        "    async def wait(self):\n",
        "        async with self._lock:\n",
        "            now = time.perf_counter()\n",
        "            delta = self._last + self.interval - now\n",
        "            if delta > 0:\n",
        "                await asyncio.sleep(delta)\n",
        "                self._last = time.perf_counter()\n",
        "            else:\n",
        "                self._last = now\n",
        "\n",
        "rate_limiter = RateLimiter(REQS_PER_SEC)\n",
        "\n",
        "# --------------------\n",
        "# Networking with retries\n",
        "# --------------------\n",
        "async def fetch(client: httpx.AsyncClient, url: str, *, max_attempts=5) -> Optional[str]:\n",
        "    backoff = 1.0\n",
        "    for attempt in range(1, max_attempts + 1):\n",
        "        await rate_limiter.wait()\n",
        "        try:\n",
        "            r = await client.get(url, timeout=TIMEOUT)\n",
        "            if r.status_code == 429:\n",
        "                ra = int(r.headers.get(\"retry-after\",\"3\"))\n",
        "                print(f\"[429] {url} – sleep {ra}s\"); await asyncio.sleep(ra); raise httpx.HTTPStatusError(\"429\", request=r.request, response=r)\n",
        "            if r.status_code >= 500:\n",
        "                raise httpx.HTTPStatusError(str(r.status_code), request=r.request, response=r)\n",
        "            r.raise_for_status()\n",
        "            return r.text\n",
        "        except Exception as e:\n",
        "            if attempt == max_attempts:\n",
        "                print(f\"[ERROR] {url} – failed after {attempt} attempts: {e}\")\n",
        "                return None\n",
        "            sl = min(backoff, 30.0) + random.uniform(0, 0.5)\n",
        "            print(f\"[WARN] {url} – attempt {attempt} failed, retry in {sl:.1f}s\")\n",
        "            await asyncio.sleep(sl)\n",
        "            backoff = min(backoff * 2, 30.0)\n",
        "\n",
        "def parse_listing(html: str) -> List[str]:\n",
        "    tree = HTMLParser(html)\n",
        "    urls = set()\n",
        "    for a in tree.css(\"h3 a[href]\"):\n",
        "        href = (a.attributes.get(\"href\") or \"\").strip()\n",
        "        if href:\n",
        "            urls.add(urljoin(BASE, href))\n",
        "    # fallback: any /article link\n",
        "    for a in tree.css(\"a[href]\"):\n",
        "        href = (a.attributes.get(\"href\") or \"\").strip()\n",
        "        if \"/article\" in href:\n",
        "            urls.add(urljoin(BASE, href))\n",
        "    return list(urls)\n",
        "\n",
        "# Optional image downloader\n",
        "async def download_image(client: httpx.AsyncClient, url: str):\n",
        "    try:\n",
        "        await rate_limiter.wait()\n",
        "        r = await client.get(url, timeout=TIMEOUT)\n",
        "        r.raise_for_status()\n",
        "        name = slugify(url)\n",
        "        ext = os.path.splitext(urlparse(url).path)[1] or \".bin\"\n",
        "        path = os.path.join(IMAGES_DIR, f\"{name}{ext}\")\n",
        "        with open(path, \"wb\") as f:\n",
        "            f.write(r.content)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "# --------------------\n",
        "# Main crawl\n",
        "# --------------------\n",
        "async def crawl():\n",
        "    seen = load_seen_urls()\n",
        "    total_saved = len(seen)\n",
        "    print(f\"[INFO] Resuming with {total_saved} articles already saved.\")\n",
        "\n",
        "    limits = httpx.Limits(max_connections=64, max_keepalive_connections=32, keepalive_expiry=30.0)\n",
        "    async with httpx.AsyncClient(http2=True, headers=HEADERS, limits=limits, follow_redirects=True) as client:\n",
        "        page = 1\n",
        "        pbar = tqdm(desc=\"Listing pages\", unit=\"page\")\n",
        "        while True:\n",
        "            if MAX_PAGES and page > MAX_PAGES:\n",
        "                break\n",
        "            list_url = listing_url(page)\n",
        "            html = await fetch(client, list_url)\n",
        "            if not html:\n",
        "                print(f\"[ERROR] listing {page} failed; stopping.\")\n",
        "                break\n",
        "\n",
        "            links = [u for u in parse_listing(html) if u not in seen]\n",
        "            if not links:\n",
        "                print(\"[INFO] No new links on this page; assuming end.\")\n",
        "                break\n",
        "\n",
        "            print(f\"[PAGE {page}] {len(links)} new links\")\n",
        "            sem = asyncio.Semaphore(CONCURRENCY)\n",
        "\n",
        "            async def process_link(aurl: str):\n",
        "                nonlocal total_saved\n",
        "                async with sem:\n",
        "                    html = await fetch(client, aurl)\n",
        "                    if not html: return\n",
        "                    art = parse_article(html, aurl)\n",
        "                    if not date_in_range(art.get(\"date_iso\")):\n",
        "                        return\n",
        "\n",
        "                    # Save snapshot HTML (clean content)\n",
        "                    snap_rel = save_snapshot_html(art)\n",
        "                    art[\"snapshot_path\"] = snap_rel\n",
        "\n",
        "                    # Optionally download images\n",
        "                    if DOWNLOAD_IMAGES and art.get(\"images\"):\n",
        "                        await asyncio.gather(*[download_image(client, im) for im in art[\"images\"]])\n",
        "\n",
        "                    # Stream to disk\n",
        "                    append_jsonl_gz(art)\n",
        "                    append_csv_row(art)\n",
        "                    add_seen_url(aurl)\n",
        "                    seen.add(aurl)\n",
        "                    total_saved += 1\n",
        "                    if total_saved % 10 == 0:\n",
        "                        print(f\"[PROGRESS] saved={total_saved}\")\n",
        "                    if total_saved % CHECKPOINT_EVERY == 0:\n",
        "                        # light sync point/log\n",
        "                        print(f\"[CHECKPOINT] {total_saved} articles stored…\")\n",
        "\n",
        "            tasks = [asyncio.create_task(process_link(u)) for u in links]\n",
        "            try:\n",
        "                for t in asyncio.as_completed(tasks):\n",
        "                    await t\n",
        "            except Exception as e:\n",
        "                # if any task errors, cancel everything to avoid leaving background tasks\n",
        "                for t in tasks:\n",
        "                    if not t.done():\n",
        "                        t.cancel()\n",
        "                # give cancellations a chance to propagate\n",
        "                await asyncio.gather(*tasks, return_exceptions=True)\n",
        "                raise e\n",
        "\n",
        "            page += 1\n",
        "            pbar.update(1)\n",
        "\n",
        "        pbar.close()\n",
        "        print(f\"[DONE] Total saved: {total_saved}\")\n",
        "        print(f\"[PATH] JSONL (gz): {JSONL_PATH}\")\n",
        "        print(f\"[PATH] CSV       : {CSV_PATH}\")\n",
        "        print(f\"[PATH] Snapshots : {PAGES_DIR}\")\n",
        "\n",
        "await crawl()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "9dc2f7ad551843d4bb16b0ba50174701",
            "c32d4750dc6e41abb7b0575238a75b62",
            "bd06b15ea863470ea2c787c3da52ed4c",
            "7108ee24712248e69b230ac77f2a4d83",
            "62ec7cb6cabf45ab8ceb1e18a09bcd06",
            "4b3601136cf34461a28b1907b4e38671",
            "321f0a2189334f06bd427d5d57ef4c5e",
            "e2b5a3d431ea43439a73fe6725a7191d",
            "be932cc4d52a4c988a39860659b6d980",
            "b5b3ffe0e617425ab7eef897817e4b49",
            "1943921901dd447ab47e7ef3c3b278f0"
          ]
        },
        "id": "lX-ye8uy0ECv",
        "outputId": "b2757596-3510-477e-a39f-cf2577b52dad"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-456' coro=<crawl.<locals>.process_link() done, defined at /tmp/ipython-input-3491645876.py:354> exception=ValueError('Bad CSS Selectors: ,')>\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3491645876.py\", line 359, in process_link\n",
            "    art = parse_article(html, aurl)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-3491645876.py\", line 161, in parse_article\n",
            "    content_html_clean, content_text = sanitize_article_container(tree, url)\n",
            "                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-3491645876.py\", line 116, in sanitize_article_container\n",
            "    for n in container.css(junk_sel):\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"selectolax/modest/node.pxi\", line 468, in selectolax.parser.Node.css\n",
            "  File \"selectolax/modest/selection.pxi\", line 161, in selectolax.parser.find_nodes\n",
            "  File \"selectolax/modest/selection.pxi\", line 20, in selectolax.parser.CSSSelector.__init__\n",
            "ValueError: Bad CSS Selectors: ,\n",
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-455' coro=<crawl.<locals>.process_link() done, defined at /tmp/ipython-input-3491645876.py:354> exception=ValueError('Bad CSS Selectors: ,')>\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3491645876.py\", line 359, in process_link\n",
            "    art = parse_article(html, aurl)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-3491645876.py\", line 161, in parse_article\n",
            "    content_html_clean, content_text = sanitize_article_container(tree, url)\n",
            "                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-3491645876.py\", line 116, in sanitize_article_container\n",
            "    for n in container.css(junk_sel):\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"selectolax/modest/node.pxi\", line 468, in selectolax.parser.Node.css\n",
            "  File \"selectolax/modest/selection.pxi\", line 161, in selectolax.parser.find_nodes\n",
            "  File \"selectolax/modest/selection.pxi\", line 20, in selectolax.parser.CSSSelector.__init__\n",
            "ValueError: Bad CSS Selectors: ,\n",
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-454' coro=<crawl.<locals>.process_link() done, defined at /tmp/ipython-input-3491645876.py:354> exception=ValueError('Bad CSS Selectors: ,')>\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3491645876.py\", line 359, in process_link\n",
            "    art = parse_article(html, aurl)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-3491645876.py\", line 161, in parse_article\n",
            "    content_html_clean, content_text = sanitize_article_container(tree, url)\n",
            "                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-3491645876.py\", line 116, in sanitize_article_container\n",
            "    for n in container.css(junk_sel):\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"selectolax/modest/node.pxi\", line 468, in selectolax.parser.Node.css\n",
            "  File \"selectolax/modest/selection.pxi\", line 161, in selectolax.parser.find_nodes\n",
            "  File \"selectolax/modest/selection.pxi\", line 20, in selectolax.parser.CSSSelector.__init__\n",
            "ValueError: Bad CSS Selectors: ,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Resuming with 67 articles already saved.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Listing pages: 0page [00:00, ?page/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9dc2f7ad551843d4bb16b0ba50174701"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] No new links on this page; assuming end.\n",
            "[DONE] Total saved: 67\n",
            "[PATH] JSONL (gz): /content/boursenews_marches_full/boursenews_marches.jsonl.gz\n",
            "[PATH] CSV       : /content/boursenews_marches_full/boursenews_marches.csv\n",
            "[PATH] Snapshots : /content/boursenews_marches_full/pages\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Boursenews \"Marchés\" – FULL CONTENT scraper (JSON-LD aware)\n",
        "# =========================\n",
        "\n",
        "!pip -q install httpx[http2] selectolax pandas python-dateutil orjson tqdm\n",
        "\n",
        "import asyncio, time, re, os, csv, gzip, orjson, json, random, html\n",
        "from urllib.parse import urljoin, urlparse\n",
        "from datetime import datetime\n",
        "from typing import Optional, List, Dict, Set\n",
        "import httpx\n",
        "from selectolax.parser import HTMLParser\n",
        "from dateutil import parser as dateparser\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "BASE = \"https://boursenews.ma\"\n",
        "SECTION = \"/articles/marches\"\n",
        "START_URL = f\"{BASE}{SECTION}\"\n",
        "\n",
        "# --------------------\n",
        "# Config\n",
        "# --------------------\n",
        "OUTPUT_DIR = \"/content/boursenews_marches_full\"\n",
        "CONCURRENCY = 24\n",
        "REQS_PER_SEC = 8\n",
        "TIMEOUT = 20.0\n",
        "MAX_PAGES: Optional[int] = None           # e.g. 40\n",
        "KEEP_ON_AFTER_DATE: Optional[str] = None  # \"2025-07-01\"\n",
        "KEEP_ON_BEFORE_DATE: Optional[str] = None # \"2025-12-31\"\n",
        "CHECKPOINT_EVERY = 50\n",
        "DOWNLOAD_IMAGES = False\n",
        "\n",
        "HEADERS = {\n",
        "    \"user-agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 \"\n",
        "                  \"KHTML, like Gecko) Chrome/124.0 Safari/537.36\"\n",
        "}\n",
        "\n",
        "# --------------------\n",
        "# Output dirs\n",
        "# --------------------\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "PAGES_DIR = os.path.join(OUTPUT_DIR, \"pages\")\n",
        "IMAGES_DIR = os.path.join(OUTPUT_DIR, \"images\")\n",
        "os.makedirs(PAGES_DIR, exist_ok=True)\n",
        "if DOWNLOAD_IMAGES:\n",
        "    os.makedirs(IMAGES_DIR, exist_ok=True)\n",
        "\n",
        "JSONL_PATH = os.path.join(OUTPUT_DIR, \"boursenews_marches.jsonl.gz\")\n",
        "CSV_PATH   = os.path.join(OUTPUT_DIR, \"boursenews_marches.csv\")\n",
        "SEEN_PATH  = os.path.join(OUTPUT_DIR, \"seen_urls.txt\")\n",
        "\n",
        "# --------------------\n",
        "# Utils\n",
        "# --------------------\n",
        "def clean_ws(s: Optional[str]) -> Optional[str]:\n",
        "    if not s: return s\n",
        "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
        "\n",
        "def parse_date_iso(text: Optional[str]) -> Optional[str]:\n",
        "    if not text: return None\n",
        "    try:\n",
        "        dt = dateparser.parse(text, dayfirst=True, fuzzy=True, languages=[\"fr\",\"en\"])\n",
        "        return dt.date().isoformat() if dt else None\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def date_in_range(date_iso: Optional[str]) -> bool:\n",
        "    if not (KEEP_ON_AFTER_DATE or KEEP_ON_BEFORE_DATE):\n",
        "        return True\n",
        "    if not date_iso:\n",
        "        return False\n",
        "    try:\n",
        "        d = datetime.fromisoformat(date_iso).date()\n",
        "        if KEEP_ON_AFTER_DATE and d < datetime.fromisoformat(KEEP_ON_AFTER_DATE).date():\n",
        "            return False\n",
        "        if KEEP_ON_BEFORE_DATE and d > datetime.fromisoformat(KEEP_ON_BEFORE_DATE).date():\n",
        "            return False\n",
        "        return True\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def listing_url(page: int) -> str:\n",
        "    return START_URL if page == 1 else f\"{START_URL}/{page}\"\n",
        "\n",
        "def slugify(url: str) -> str:\n",
        "    path = urlparse(url).path.strip(\"/\").replace(\"/\", \"-\")\n",
        "    path = re.sub(r\"[^a-zA-Z0-9\\-_.]\", \"-\", path)[:140]\n",
        "    return path or \"article\"\n",
        "\n",
        "def append_jsonl_gz(obj: dict):\n",
        "    with gzip.open(JSONL_PATH, \"ab\") as f:\n",
        "        f.write(orjson.dumps(obj))\n",
        "        f.write(b\"\\n\")\n",
        "\n",
        "def append_csv_row(row: dict, fields=(\"url\",\"title\",\"date_text\",\"date_iso\",\"author\",\"snapshot_path\",\"images_count\")):\n",
        "    header_needed = not os.path.exists(CSV_PATH)\n",
        "    data = {\n",
        "        \"url\": row.get(\"url\"),\n",
        "        \"title\": row.get(\"title\"),\n",
        "        \"date_text\": row.get(\"date_text\"),\n",
        "        \"date_iso\": row.get(\"date_iso\"),\n",
        "        \"author\": row.get(\"author\"),\n",
        "        \"snapshot_path\": row.get(\"snapshot_path\"),\n",
        "        \"images_count\": len(row.get(\"images\") or []) if isinstance(row.get(\"images\"), list) else 0\n",
        "    }\n",
        "    with open(CSV_PATH, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        w = csv.DictWriter(f, fieldnames=list(fields))\n",
        "        if header_needed:\n",
        "            w.writeheader()\n",
        "        w.writerow(data)\n",
        "\n",
        "def load_seen_urls() -> Set[str]:\n",
        "    if not os.path.exists(SEEN_PATH):\n",
        "        return set()\n",
        "    with open(SEEN_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "        return set(line.strip() for line in f if line.strip())\n",
        "\n",
        "def add_seen_url(u: str):\n",
        "    with open(SEEN_PATH, \"a\", encoding=\"utf-8\") as f:\n",
        "        f.write(u + \"\\n\")\n",
        "\n",
        "def save_snapshot_html(article: dict):\n",
        "    slug = slugify(article[\"url\"])\n",
        "    fname = f\"{slug}.html\"\n",
        "    fpath = os.path.join(PAGES_DIR, fname)\n",
        "    title = article.get(\"title\") or slug\n",
        "    body = article.get(\"content_html_clean\") or \"\"\n",
        "    date_txt = article.get(\"date_text\") or \"\"\n",
        "    html_doc = f\"\"\"<!doctype html>\n",
        "<html lang=\"fr\"><head><meta charset=\"utf-8\">\n",
        "<title>{title}</title>\n",
        "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n",
        "<style>body{{font-family:system-ui,-apple-system,Segoe UI,Roboto;max-width:820px;margin:40px auto;padding:0 16px;line-height:1.6}}</style>\n",
        "</head><body>\n",
        "<h1>{title}</h1>\n",
        "<p><em>{date_txt}</em></p>\n",
        "<hr/>\n",
        "{body}\n",
        "</body></html>\"\"\"\n",
        "    with open(fpath, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(html_doc)\n",
        "    return os.path.relpath(fpath, OUTPUT_DIR)\n",
        "\n",
        "# --------------------\n",
        "# Networking + rate limit\n",
        "# --------------------\n",
        "class RateLimiter:\n",
        "    def __init__(self, rps: float):\n",
        "        self.interval = 1.0 / max(0.01, rps)\n",
        "        self._last = 0.0\n",
        "        self._lock = asyncio.Lock()\n",
        "    async def wait(self):\n",
        "        async with self._lock:\n",
        "            now = time.perf_counter()\n",
        "            delta = self._last + self.interval - now\n",
        "            if delta > 0:\n",
        "                await asyncio.sleep(delta)\n",
        "                self._last = time.perf_counter()\n",
        "            else:\n",
        "                self._last = now\n",
        "\n",
        "rate_limiter = RateLimiter(REQS_PER_SEC)\n",
        "\n",
        "async def fetch(client: httpx.AsyncClient, url: str, *, max_attempts=5) -> Optional[str]:\n",
        "    backoff = 1.0\n",
        "    for attempt in range(1, max_attempts + 1):\n",
        "        await rate_limiter.wait()\n",
        "        try:\n",
        "            r = await client.get(url, timeout=TIMEOUT)\n",
        "            if r.status_code == 429:\n",
        "                ra = int(r.headers.get(\"retry-after\",\"3\"))\n",
        "                print(f\"[429] {url} – sleep {ra}s\"); await asyncio.sleep(ra); raise httpx.HTTPStatusError(\"429\", request=r.request, response=r)\n",
        "            if r.status_code >= 500:\n",
        "                raise httpx.HTTPStatusError(str(r.status_code), request=r.request, response=r)\n",
        "            r.raise_for_status()\n",
        "            return r.text\n",
        "        except Exception as e:\n",
        "            if attempt == max_attempts:\n",
        "                print(f\"[ERROR] {url} – failed after {attempt} attempts: {e}\")\n",
        "                return None\n",
        "            sl = min(backoff, 30.0) + random.uniform(0, 0.5)\n",
        "            print(f\"[WARN] {url} – attempt {attempt} failed, retry in {sl:.1f}s\")\n",
        "            await asyncio.sleep(sl)\n",
        "            backoff = min(backoff * 2, 30.0)\n",
        "\n",
        "def parse_listing(html: str) -> List[str]:\n",
        "    tree = HTMLParser(html)\n",
        "    urls = set()\n",
        "    for a in tree.css(\"h3 a[href]\"):\n",
        "        href = (a.attributes.get(\"href\") or \"\").strip()\n",
        "        if \"/article/\" in href:\n",
        "            urls.add(urljoin(BASE, href))\n",
        "    if not urls:\n",
        "        for a in tree.css(\"a[href]\"):\n",
        "            href = (a.attributes.get(\"href\") or \"\").strip()\n",
        "            if \"/article/\" in href:\n",
        "                urls.add(urljoin(BASE, href))\n",
        "    return list(urls)\n",
        "\n",
        "# --------------------\n",
        "# Article parsing (JSON-LD first)\n",
        "# --------------------\n",
        "def extract_jsonld_article(tree: HTMLParser) -> dict:\n",
        "    out = {}\n",
        "    for s in tree.css('script[type=\"application/ld+json\"]'):\n",
        "        try:\n",
        "            raw = s.text() or \"\"\n",
        "            data = json.loads(raw)\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "        def to_list(x):\n",
        "            if isinstance(x, list): return x\n",
        "            return [x]\n",
        "\n",
        "        for obj in to_list(data):\n",
        "            if not isinstance(obj, dict):\n",
        "                continue\n",
        "            t = obj.get(\"@type\") or obj.get(\"@context\")\n",
        "            # NewsArticle object\n",
        "            if (isinstance(t, str) and \"NewsArticle\" in t) or obj.get(\"@type\") == \"NewsArticle\":\n",
        "                out[\"headline\"] = clean_ws(obj.get(\"headline\"))\n",
        "                out[\"datePublished\"] = clean_ws(obj.get(\"datePublished\"))\n",
        "                out[\"dateModified\"] = clean_ws(obj.get(\"dateModified\"))\n",
        "                art_body = obj.get(\"articleBody\")\n",
        "                if isinstance(art_body, str):\n",
        "                    # JSON-LD on this site often contains HTML-escaped text\n",
        "                    out[\"articleBody\"] = html.unescape(art_body)\n",
        "                # stop after the first good one\n",
        "                return out\n",
        "    return out\n",
        "\n",
        "def parse_article(html_text: str, url: str) -> Dict:\n",
        "    # default structure\n",
        "    result = {\n",
        "        \"url\": url, \"title\": None, \"date_text\": None, \"date_iso\": None,\n",
        "        \"author\": None, \"images\": None, \"content_text\": None, \"content_html_clean\": None\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        tree = HTMLParser(html_text)\n",
        "    except Exception:\n",
        "        return result\n",
        "\n",
        "    # 1) JSON-LD first\n",
        "    jl = extract_jsonld_article(tree)\n",
        "    if jl.get(\"headline\"):\n",
        "        result[\"title\"] = jl[\"headline\"]\n",
        "    if jl.get(\"datePublished\"):\n",
        "        result[\"date_text\"] = jl[\"datePublished\"]\n",
        "        result[\"date_iso\"] = parse_date_iso(jl[\"datePublished\"])\n",
        "    # articleBody from JSON-LD becomes a fallback text if DOM body fails\n",
        "    jsonld_body_text = jl.get(\"articleBody\")\n",
        "\n",
        "    # 2) DOM title/date fallback (site-specific)\n",
        "    if not result[\"title\"]:\n",
        "        # .text_article_la_une h2 (with inner <a>)\n",
        "        tnode = tree.css_first(\".text_article_la_une\")\n",
        "        if tnode:\n",
        "            h2 = tnode.css_first(\"h2\")\n",
        "            if h2:\n",
        "                if h2.css_first(\"a\") and h2.css_first(\"a\").text():\n",
        "                    result[\"title\"] = clean_ws(h2.css_first(\"a\").text())\n",
        "                elif h2.text():\n",
        "                    result[\"title\"] = clean_ws(h2.text())\n",
        "        if not result[\"title\"]:\n",
        "            # other fallbacks\n",
        "            for sel in (\"h1.article-title\", \".post-title h1\", \"h1\", \"title\"):\n",
        "                n = tree.css_first(sel)\n",
        "                if n and n.text():\n",
        "                    result[\"title\"] = clean_ws(n.text())\n",
        "                    break\n",
        "\n",
        "    if not result[\"date_text\"]:\n",
        "        dnode = tree.css_first(\".text_article_la_une\")\n",
        "        if dnode:\n",
        "            sp = dnode.css_first(\"span\")\n",
        "            if sp and sp.text():\n",
        "                result[\"date_text\"] = clean_ws(sp.text())\n",
        "                result[\"date_iso\"] = parse_date_iso(result[\"date_text\"])\n",
        "\n",
        "    # 3) BODY: keep only .article_detail_description\n",
        "    body = tree.css_first(\".article_detail_description\")\n",
        "    if not body:\n",
        "        # guarded fallbacks (avoid site chrome)\n",
        "        for sel in (\".entry-content\", \".article-content\", \"article\"):\n",
        "            body = tree.css_first(sel)\n",
        "            if body and body.css_first(\"p\"):\n",
        "                break\n",
        "    if not body:\n",
        "        body = tree.body or tree\n",
        "\n",
        "    # remove junk (no comma-joined selectors)\n",
        "    for tag in (\"script\", \"style\", \"nav\", \"aside\", \"iframe\", \"form\", \"noscript\"):\n",
        "        for n in body.css(tag):\n",
        "            n.decompose()\n",
        "\n",
        "    # absolutize links\n",
        "    for el in body.css(\"[href]\"):\n",
        "        val = (el.attributes.get(\"href\") or \"\").strip()\n",
        "        if val:\n",
        "            el.attributes[\"href\"] = urljoin(url, val)\n",
        "    for el in body.css(\"[src]\"):\n",
        "        val = (el.attributes.get(\"src\") or \"\").strip()\n",
        "        if val:\n",
        "            el.attributes[\"src\"] = urljoin(url, val)\n",
        "\n",
        "    # full cleaned HTML (with tables, lists, inline styles retained)\n",
        "    result[\"content_html_clean\"] = body.html or None\n",
        "\n",
        "    # plain text: from p and li only (keeps narrative, skips nav)\n",
        "    parts = []\n",
        "    for tag in (\"p\", \"li\"):\n",
        "        for n in body.css(tag):\n",
        "            t = clean_ws(n.text() or \"\")\n",
        "            if t and len(t) > 2:\n",
        "                parts.append(t)\n",
        "    if parts:\n",
        "        result[\"content_text\"] = \"\\n\\n\".join(parts)\n",
        "    elif jsonld_body_text:\n",
        "        # fallback to JSON-LD articleBody plain text\n",
        "        result[\"content_text\"] = clean_ws(html.unescape(jsonld_body_text))\n",
        "\n",
        "    # images from body\n",
        "    images = []\n",
        "    if result[\"content_html_clean\"]:\n",
        "        try:\n",
        "            t2 = HTMLParser(result[\"content_html_clean\"])\n",
        "            for img in t2.css(\"img[src]\"):\n",
        "                s = (img.attributes.get(\"src\") or \"\").strip()\n",
        "                if s:\n",
        "                    images.append(s)\n",
        "            # dedupe\n",
        "            seen = set(); out=[]\n",
        "            for s in images:\n",
        "                if s not in seen:\n",
        "                    seen.add(s); out.append(s)\n",
        "            images = out\n",
        "        except Exception:\n",
        "            images = None\n",
        "    result[\"images\"] = images or None\n",
        "\n",
        "    # author (heuristic “- par …” inside date_text)\n",
        "    if result[\"date_text\"]:\n",
        "        m = re.search(r\"\\s+-\\s*par\\s+(.*)$\", result[\"date_text\"], flags=re.I)\n",
        "        if m:\n",
        "            result[\"author\"] = m.group(1).strip()\n",
        "\n",
        "    return result\n",
        "\n",
        "# --------------------\n",
        "# Optional image downloader\n",
        "# --------------------\n",
        "async def download_image(client: httpx.AsyncClient, url: str):\n",
        "    try:\n",
        "        await rate_limiter.wait()\n",
        "        r = await client.get(url, timeout=TIMEOUT)\n",
        "        r.raise_for_status()\n",
        "        name = slugify(url)\n",
        "        ext = os.path.splitext(urlparse(url).path)[1] or \".bin\"\n",
        "        path = os.path.join(IMAGES_DIR, f\"{name}{ext}\")\n",
        "        with open(path, \"wb\") as f:\n",
        "            f.write(r.content)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "# --------------------\n",
        "# Crawl\n",
        "# --------------------\n",
        "async def crawl():\n",
        "    seen = load_seen_urls()\n",
        "    total_saved = len(seen)\n",
        "    print(f\"[INFO] Resuming with {total_saved} articles already saved.\")\n",
        "\n",
        "    limits = httpx.Limits(max_connections=64, max_keepalive_connections=32, keepalive_expiry=30.0)\n",
        "    async with httpx.AsyncClient(http2=True, headers=HEADERS, limits=limits, follow_redirects=True) as client:\n",
        "        page = 1\n",
        "        pbar = tqdm(desc=\"Listing pages\", unit=\"page\")\n",
        "        EMPTY_PAGES_LIMIT = 2  # stop after 2 pages with zero article links\n",
        "        empty_streak = 0\n",
        "\n",
        "        while True:\n",
        "            if MAX_PAGES and page > MAX_PAGES:\n",
        "                break\n",
        "\n",
        "            list_url = listing_url(page)\n",
        "            html_page = await fetch(client, list_url)\n",
        "            if not html_page:\n",
        "                print(f\"[ERROR] listing {page} failed; stopping.\")\n",
        "                break\n",
        "\n",
        "            # Find all article links on this page (not just new ones)\n",
        "            all_links = parse_listing(html_page)\n",
        "            if not all_links:\n",
        "                empty_streak += 1\n",
        "                print(f\"[INFO] Page {page} has no article links (empty_streak={empty_streak}).\")\n",
        "                if empty_streak >= EMPTY_PAGES_LIMIT:\n",
        "                    print(\"[INFO] Reached empty pages limit. Stopping.\")\n",
        "                    break\n",
        "                page += 1\n",
        "                pbar.update(1)\n",
        "                continue\n",
        "\n",
        "            empty_streak = 0\n",
        "            links = [u for u in all_links if u not in seen]\n",
        "            print(f\"[PAGE {page}] total={len(all_links)} new={len(links)} (seen={len(seen)})\")\n",
        "\n",
        "            if links:\n",
        "                sem = asyncio.Semaphore(CONCURRENCY)\n",
        "\n",
        "                async def process_link(aurl: str):\n",
        "                    nonlocal total_saved\n",
        "                    async with sem:\n",
        "                        art_html = await fetch(client, aurl)\n",
        "                        if not art_html:\n",
        "                            return\n",
        "                        art = parse_article(art_html, aurl)\n",
        "                        if not date_in_range(art.get(\"date_iso\")):\n",
        "                            return\n",
        "\n",
        "                        snap_rel = save_snapshot_html(art)\n",
        "                        art[\"snapshot_path\"] = snap_rel\n",
        "\n",
        "                        if DOWNLOAD_IMAGES and art.get(\"images\"):\n",
        "                            await asyncio.gather(*[download_image(client, im) for im in art[\"images\"]])\n",
        "\n",
        "                        append_jsonl_gz(art)\n",
        "                        append_csv_row(art)\n",
        "                        add_seen_url(aurl)\n",
        "                        seen.add(aurl)\n",
        "                        total_saved += 1\n",
        "                        if total_saved % 10 == 0:\n",
        "                            print(f\"[PROGRESS] saved={total_saved}\")\n",
        "                        if total_saved % CHECKPOINT_EVERY == 0:\n",
        "                            print(f\"[CHECKPOINT] {total_saved} articles stored…\")\n",
        "\n",
        "                # Structured concurrency: all tasks complete/cancel before leaving the block\n",
        "                try:\n",
        "                    tg = asyncio.TaskGroup()\n",
        "                    async with tg:\n",
        "                        for u in links:\n",
        "                            tg.create_task(process_link(u))\n",
        "                except* Exception as eg:\n",
        "                    # Non-fatal: continue to next page\n",
        "                    print(f\"[ERROR] A task failed on page {page}: {eg.exceptions[0]}\")\n",
        "\n",
        "            # advance to next listing page even if there were 0 new links\n",
        "            page += 1\n",
        "            pbar.update(1)\n",
        "\n",
        "        pbar.close()\n",
        "\n",
        "        print(f\"[DONE] Total saved: {total_saved}\")\n",
        "        print(f\"[PATH] JSONL (gz): {JSONL_PATH}\")\n",
        "        print(f\"[PATH] CSV       : {CSV_PATH}\")\n",
        "        print(f\"[PATH] Snapshots : {PAGES_DIR}\")\n",
        "\n",
        "await crawl()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "9277f433937d43578b6dc14673e0f971",
            "9021ed39b91f480f8cb195c6b7724e64",
            "55cc75f64c0f46e5bca31a345f120af2",
            "4fb0d2bda2474a50a53bf77a8592bc57",
            "50ac0a5207cd45f9945f22df1fd92e23",
            "f9b3110c6acb49ab8206798ab647f55d",
            "786461cca19840efa5f147bc41dd66d1",
            "7f6578b4221e44c9b20e7de121e5cdb5",
            "d4fe4a1063b743738d4ac1346874bb15",
            "7cb7b78d80564367b9fdd91e13b88c06",
            "9148514e5b85481b88bbf42537392348"
          ]
        },
        "id": "YLxPzqUc2liD",
        "outputId": "a242c2d1-6745-4ad2-ba6a-9a6870c3c429"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Resuming with 67 articles already saved.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Listing pages: 0page [00:00, ?page/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9277f433937d43578b6dc14673e0f971"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PAGE 1] total=10 new=0 (seen=67)\n",
            "[PAGE 2] total=10 new=0 (seen=67)\n",
            "[PAGE 3] total=10 new=0 (seen=67)\n",
            "[PAGE 4] total=10 new=0 (seen=67)\n",
            "[PAGE 5] total=10 new=10 (seen=67)\n",
            "[PROGRESS] saved=70\n",
            "[PAGE 6] total=10 new=10 (seen=77)\n",
            "[PROGRESS] saved=80\n",
            "[PAGE 7] total=10 new=10 (seen=87)\n",
            "[PROGRESS] saved=90\n",
            "[PAGE 8] total=10 new=10 (seen=97)\n",
            "[PROGRESS] saved=100\n",
            "[CHECKPOINT] 100 articles stored…\n",
            "[PAGE 9] total=10 new=10 (seen=107)\n",
            "[PROGRESS] saved=110\n",
            "[PAGE 10] total=10 new=10 (seen=117)\n",
            "[PROGRESS] saved=120\n",
            "[PAGE 11] total=10 new=10 (seen=127)\n",
            "[PROGRESS] saved=130\n",
            "[PAGE 12] total=10 new=10 (seen=137)\n",
            "[PROGRESS] saved=140\n",
            "[PAGE 13] total=10 new=10 (seen=147)\n",
            "[PROGRESS] saved=150\n",
            "[CHECKPOINT] 150 articles stored…\n",
            "[PAGE 14] total=10 new=10 (seen=157)\n",
            "[PROGRESS] saved=160\n",
            "[PAGE 15] total=10 new=10 (seen=167)\n",
            "[PROGRESS] saved=170\n",
            "[PAGE 16] total=10 new=10 (seen=177)\n",
            "[PROGRESS] saved=180\n",
            "[PAGE 17] total=10 new=10 (seen=187)\n",
            "[PROGRESS] saved=190\n",
            "[PAGE 18] total=10 new=10 (seen=197)\n",
            "[PROGRESS] saved=200\n",
            "[CHECKPOINT] 200 articles stored…\n",
            "[PAGE 19] total=10 new=10 (seen=207)\n",
            "[PROGRESS] saved=210\n",
            "[PAGE 20] total=10 new=10 (seen=217)\n",
            "[PROGRESS] saved=220\n",
            "[PAGE 21] total=10 new=10 (seen=227)\n",
            "[PROGRESS] saved=230\n",
            "[PAGE 22] total=10 new=10 (seen=237)\n",
            "[PROGRESS] saved=240\n",
            "[PAGE 23] total=10 new=10 (seen=247)\n",
            "[PROGRESS] saved=250\n",
            "[CHECKPOINT] 250 articles stored…\n",
            "[PAGE 24] total=10 new=10 (seen=257)\n",
            "[PROGRESS] saved=260\n",
            "[PAGE 25] total=10 new=10 (seen=267)\n",
            "[PROGRESS] saved=270\n",
            "[PAGE 26] total=10 new=10 (seen=277)\n",
            "[PROGRESS] saved=280\n",
            "[PAGE 27] total=10 new=10 (seen=287)\n",
            "[PROGRESS] saved=290\n",
            "[PAGE 28] total=10 new=10 (seen=297)\n",
            "[PROGRESS] saved=300\n",
            "[CHECKPOINT] 300 articles stored…\n",
            "[PAGE 29] total=10 new=10 (seen=307)\n",
            "[PROGRESS] saved=310\n",
            "[PAGE 30] total=10 new=10 (seen=317)\n",
            "[PROGRESS] saved=320\n",
            "[PAGE 31] total=10 new=10 (seen=327)\n",
            "[PROGRESS] saved=330\n",
            "[WARN] https://boursenews.ma/article/marches/marsa-maroc-recommandation-bkgr-mai-2025 – attempt 1 failed, retry in 1.2s\n",
            "[PAGE 32] total=10 new=10 (seen=337)\n",
            "[PROGRESS] saved=340\n",
            "[PAGE 33] total=10 new=10 (seen=347)\n",
            "[PROGRESS] saved=350\n",
            "[CHECKPOINT] 350 articles stored…\n",
            "[PAGE 34] total=10 new=10 (seen=357)\n",
            "[PROGRESS] saved=360\n",
            "[PAGE 35] total=10 new=10 (seen=367)\n",
            "[PROGRESS] saved=370\n",
            "[PAGE 36] total=10 new=10 (seen=377)\n",
            "[PROGRESS] saved=380\n",
            "[PAGE 37] total=10 new=10 (seen=387)\n",
            "[PROGRESS] saved=390\n",
            "[PAGE 38] total=10 new=10 (seen=397)\n",
            "[PROGRESS] saved=400\n",
            "[CHECKPOINT] 400 articles stored…\n",
            "[PAGE 39] total=10 new=10 (seen=407)\n",
            "[PROGRESS] saved=410\n",
            "[PAGE 40] total=10 new=10 (seen=417)\n",
            "[PROGRESS] saved=420\n",
            "[PAGE 41] total=10 new=10 (seen=427)\n",
            "[PROGRESS] saved=430\n",
            "[PAGE 42] total=10 new=10 (seen=437)\n",
            "[PROGRESS] saved=440\n",
            "[PAGE 43] total=10 new=10 (seen=447)\n",
            "[PROGRESS] saved=450\n",
            "[CHECKPOINT] 450 articles stored…\n",
            "[PAGE 44] total=10 new=10 (seen=457)\n",
            "[PROGRESS] saved=460\n",
            "[PAGE 45] total=10 new=10 (seen=467)\n",
            "[PROGRESS] saved=470\n",
            "[PAGE 46] total=10 new=10 (seen=477)\n",
            "[PROGRESS] saved=480\n",
            "[PAGE 47] total=10 new=10 (seen=487)\n",
            "[WARN] https://boursenews.ma/article/marches/guerre-commerciale-HCP – attempt 1 failed, retry in 1.1s\n",
            "[PROGRESS] saved=490\n",
            "[PAGE 48] total=10 new=10 (seen=497)\n",
            "[PROGRESS] saved=500\n",
            "[CHECKPOINT] 500 articles stored…\n",
            "[PAGE 49] total=10 new=10 (seen=507)\n",
            "[PROGRESS] saved=510\n",
            "[PAGE 50] total=10 new=10 (seen=517)\n",
            "[PROGRESS] saved=520\n",
            "[PAGE 51] total=10 new=10 (seen=527)\n",
            "[PROGRESS] saved=530\n",
            "[PAGE 52] total=10 new=10 (seen=537)\n",
            "[PROGRESS] saved=540\n",
            "[PAGE 53] total=10 new=10 (seen=547)\n",
            "[PROGRESS] saved=550\n",
            "[CHECKPOINT] 550 articles stored…\n",
            "[PAGE 54] total=10 new=10 (seen=557)\n",
            "[PROGRESS] saved=560\n",
            "[PAGE 55] total=10 new=10 (seen=567)\n",
            "[PROGRESS] saved=570\n",
            "[PAGE 56] total=10 new=10 (seen=577)\n",
            "[PROGRESS] saved=580\n",
            "[PAGE 57] total=10 new=10 (seen=587)\n",
            "[PROGRESS] saved=590\n",
            "[PAGE 58] total=10 new=10 (seen=597)\n",
            "[PROGRESS] saved=600\n",
            "[CHECKPOINT] 600 articles stored…\n",
            "[PAGE 59] total=10 new=10 (seen=607)\n",
            "[PROGRESS] saved=610\n",
            "[PAGE 60] total=10 new=10 (seen=617)\n",
            "[PROGRESS] saved=620\n",
            "[PAGE 61] total=10 new=10 (seen=627)\n",
            "[PROGRESS] saved=630\n",
            "[PAGE 62] total=10 new=10 (seen=637)\n",
            "[PROGRESS] saved=640\n",
            "[PAGE 63] total=10 new=10 (seen=647)\n",
            "[PROGRESS] saved=650\n",
            "[CHECKPOINT] 650 articles stored…\n",
            "[PAGE 64] total=10 new=10 (seen=657)\n",
            "[PROGRESS] saved=660\n",
            "[PAGE 65] total=10 new=10 (seen=667)\n",
            "[PROGRESS] saved=670\n",
            "[PAGE 66] total=10 new=10 (seen=677)\n",
            "[PROGRESS] saved=680\n",
            "[PAGE 67] total=10 new=10 (seen=687)\n",
            "[PROGRESS] saved=690\n",
            "[PAGE 68] total=10 new=10 (seen=697)\n",
            "[PROGRESS] saved=700\n",
            "[CHECKPOINT] 700 articles stored…\n",
            "[PAGE 69] total=10 new=10 (seen=707)\n",
            "[WARN] https://boursenews.ma/article/marches/office-des-changes-IDE-janvier-25 – attempt 1 failed, retry in 1.3s\n",
            "[PROGRESS] saved=710\n",
            "[PAGE 70] total=10 new=10 (seen=717)\n",
            "[PROGRESS] saved=720\n",
            "[PAGE 71] total=10 new=10 (seen=727)\n",
            "[PROGRESS] saved=730\n",
            "[PAGE 72] total=10 new=10 (seen=737)\n",
            "[PROGRESS] saved=740\n",
            "[PAGE 73] total=10 new=10 (seen=747)\n",
            "[PROGRESS] saved=750\n",
            "[CHECKPOINT] 750 articles stored…\n",
            "[PAGE 74] total=10 new=10 (seen=757)\n",
            "[PROGRESS] saved=760\n",
            "[PAGE 75] total=10 new=10 (seen=767)\n",
            "[PROGRESS] saved=770\n",
            "[PAGE 76] total=10 new=10 (seen=777)\n",
            "[PROGRESS] saved=780\n",
            "[PAGE 77] total=10 new=10 (seen=787)\n",
            "[PROGRESS] saved=790\n",
            "[PAGE 78] total=10 new=10 (seen=797)\n",
            "[PROGRESS] saved=800\n",
            "[CHECKPOINT] 800 articles stored…\n",
            "[PAGE 79] total=10 new=10 (seen=807)\n",
            "[PROGRESS] saved=810\n",
            "[PAGE 80] total=10 new=10 (seen=817)\n",
            "[PROGRESS] saved=820\n",
            "[PAGE 81] total=10 new=10 (seen=827)\n",
            "[PROGRESS] saved=830\n",
            "[PAGE 82] total=10 new=10 (seen=837)\n",
            "[PROGRESS] saved=840\n",
            "[PAGE 83] total=10 new=10 (seen=847)\n",
            "[PROGRESS] saved=850\n",
            "[CHECKPOINT] 850 articles stored…\n",
            "[PAGE 84] total=10 new=10 (seen=857)\n",
            "[PROGRESS] saved=860\n",
            "[PAGE 85] total=10 new=10 (seen=867)\n",
            "[PROGRESS] saved=870\n",
            "[PAGE 86] total=10 new=10 (seen=877)\n",
            "[PROGRESS] saved=880\n",
            "[PAGE 87] total=10 new=10 (seen=887)\n",
            "[PROGRESS] saved=890\n",
            "[PAGE 88] total=10 new=10 (seen=897)\n",
            "[PROGRESS] saved=900\n",
            "[CHECKPOINT] 900 articles stored…\n",
            "[PAGE 89] total=10 new=10 (seen=907)\n",
            "[PROGRESS] saved=910\n",
            "[PAGE 90] total=10 new=10 (seen=917)\n",
            "[PROGRESS] saved=920\n",
            "[PAGE 91] total=10 new=10 (seen=927)\n",
            "[PROGRESS] saved=930\n",
            "[PAGE 92] total=10 new=10 (seen=937)\n",
            "[PROGRESS] saved=940\n",
            "[PAGE 93] total=10 new=10 (seen=947)\n",
            "[PROGRESS] saved=950\n",
            "[CHECKPOINT] 950 articles stored…\n",
            "[PAGE 94] total=10 new=10 (seen=957)\n",
            "[PROGRESS] saved=960\n",
            "[PAGE 95] total=10 new=9 (seen=967)\n",
            "[PROGRESS] saved=970\n",
            "[PAGE 96] total=10 new=10 (seen=976)\n",
            "[PROGRESS] saved=980\n",
            "[PAGE 97] total=10 new=10 (seen=986)\n",
            "[PROGRESS] saved=990\n",
            "[PAGE 98] total=10 new=10 (seen=996)\n",
            "[PROGRESS] saved=1000\n",
            "[CHECKPOINT] 1000 articles stored…\n",
            "[PAGE 99] total=10 new=10 (seen=1006)\n",
            "[PROGRESS] saved=1010\n",
            "[WARN] https://boursenews.ma/article/marches/le-retour-de-trump-vu-par-jouahri – attempt 1 failed, retry in 1.5s\n",
            "[WARN] https://boursenews.ma/article/marches/wineo-cif – attempt 1 failed, retry in 1.2s\n",
            "[WARN] https://boursenews.ma/article/marches/assouplissement-monetaire-jouahri-toujours-prudent – attempt 1 failed, retry in 1.4s\n",
            "[PAGE 100] total=10 new=10 (seen=1016)\n",
            "[PROGRESS] saved=1020\n",
            "[PAGE 101] total=10 new=10 (seen=1026)\n",
            "[PROGRESS] saved=1030\n",
            "[PAGE 102] total=10 new=10 (seen=1036)\n",
            "[PROGRESS] saved=1040\n",
            "[PAGE 103] total=10 new=10 (seen=1046)\n",
            "[PROGRESS] saved=1050\n",
            "[CHECKPOINT] 1050 articles stored…\n",
            "[PAGE 104] total=10 new=10 (seen=1056)\n",
            "[PROGRESS] saved=1060\n",
            "[PAGE 105] total=10 new=10 (seen=1066)\n",
            "[PROGRESS] saved=1070\n",
            "[PAGE 106] total=10 new=10 (seen=1076)\n",
            "[PROGRESS] saved=1080\n",
            "[PAGE 107] total=10 new=10 (seen=1086)\n",
            "[PROGRESS] saved=1090\n",
            "[PAGE 108] total=10 new=10 (seen=1096)\n",
            "[PROGRESS] saved=1100\n",
            "[CHECKPOINT] 1100 articles stored…\n",
            "[PAGE 109] total=10 new=10 (seen=1106)\n",
            "[PROGRESS] saved=1110\n",
            "[PAGE 110] total=10 new=10 (seen=1116)\n",
            "[PROGRESS] saved=1120\n",
            "[PAGE 111] total=10 new=10 (seen=1126)\n",
            "[PROGRESS] saved=1130\n",
            "[PAGE 112] total=10 new=10 (seen=1136)\n",
            "[WARN] https://boursenews.ma/article/marches/CFG-Bank-Sogecapital-Bourse-recommande-le-titre – attempt 1 failed, retry in 1.5s\n",
            "[WARN] https://boursenews.ma/article/marches/Bourse-volumes-particuliers – attempt 1 failed, retry in 1.1s\n",
            "[WARN] https://boursenews.ma/article/marches/bourse-de-casablanca-a-un-souffle-de-l-histoire – attempt 1 failed, retry in 1.3s\n",
            "[WARN] https://boursenews.ma/article/marches/conferences-salon-21-novembre – attempt 1 failed, retry in 1.2s\n",
            "[WARN] https://boursenews.ma/article/marches/bourse-de-casablanca-masi-record-historique – attempt 1 failed, retry in 1.4s\n",
            "[PROGRESS] saved=1140\n",
            "[PAGE 113] total=10 new=10 (seen=1146)\n",
            "[PROGRESS] saved=1150\n",
            "[CHECKPOINT] 1150 articles stored…\n",
            "[PAGE 114] total=10 new=10 (seen=1156)\n",
            "[PROGRESS] saved=1160\n",
            "[PAGE 115] total=10 new=10 (seen=1166)\n",
            "[PROGRESS] saved=1170\n",
            "[PAGE 116] total=10 new=10 (seen=1176)\n",
            "[PROGRESS] saved=1180\n",
            "[PAGE 117] total=10 new=10 (seen=1186)\n",
            "[PROGRESS] saved=1190\n",
            "[PAGE 118] total=10 new=10 (seen=1196)\n",
            "[PROGRESS] saved=1200\n",
            "[CHECKPOINT] 1200 articles stored…\n",
            "[PAGE 119] total=10 new=10 (seen=1206)\n",
            "[PROGRESS] saved=1210\n",
            "[PAGE 120] total=10 new=10 (seen=1216)\n",
            "[PROGRESS] saved=1220\n",
            "[PAGE 121] total=10 new=10 (seen=1226)\n",
            "[PROGRESS] saved=1230\n",
            "[PAGE 122] total=10 new=10 (seen=1236)\n",
            "[PROGRESS] saved=1240\n",
            "[PAGE 123] total=10 new=10 (seen=1246)\n",
            "[PROGRESS] saved=1250\n",
            "[CHECKPOINT] 1250 articles stored…\n",
            "[PAGE 124] total=10 new=10 (seen=1256)\n",
            "[PROGRESS] saved=1260\n",
            "[PAGE 125] total=10 new=10 (seen=1266)\n",
            "[PROGRESS] saved=1270\n",
            "[PAGE 126] total=10 new=10 (seen=1276)\n",
            "[PROGRESS] saved=1280\n",
            "[PAGE 127] total=10 new=10 (seen=1286)\n",
            "[PROGRESS] saved=1290\n",
            "[PAGE 128] total=10 new=10 (seen=1296)\n",
            "[PROGRESS] saved=1300\n",
            "[CHECKPOINT] 1300 articles stored…\n",
            "[PAGE 129] total=10 new=10 (seen=1306)\n",
            "[PROGRESS] saved=1310\n",
            "[PAGE 130] total=10 new=10 (seen=1316)\n",
            "[PROGRESS] saved=1320\n",
            "[PAGE 131] total=10 new=10 (seen=1326)\n",
            "[PROGRESS] saved=1330\n",
            "[PAGE 132] total=10 new=10 (seen=1336)\n",
            "[PROGRESS] saved=1340\n",
            "[PAGE 133] total=10 new=10 (seen=1346)\n",
            "[PROGRESS] saved=1350\n",
            "[CHECKPOINT] 1350 articles stored…\n",
            "[PAGE 134] total=10 new=10 (seen=1356)\n",
            "[PROGRESS] saved=1360\n",
            "[PAGE 135] total=10 new=10 (seen=1366)\n",
            "[PROGRESS] saved=1370\n",
            "[PAGE 136] total=10 new=10 (seen=1376)\n",
            "[PROGRESS] saved=1380\n",
            "[PAGE 137] total=10 new=10 (seen=1386)\n",
            "[PROGRESS] saved=1390\n",
            "[PAGE 138] total=10 new=10 (seen=1396)\n",
            "[PROGRESS] saved=1400\n",
            "[CHECKPOINT] 1400 articles stored…\n",
            "[PAGE 139] total=10 new=10 (seen=1406)\n",
            "[PROGRESS] saved=1410\n",
            "[PAGE 140] total=10 new=10 (seen=1416)\n",
            "[PROGRESS] saved=1420\n",
            "[PAGE 141] total=10 new=10 (seen=1426)\n",
            "[PROGRESS] saved=1430\n",
            "[PAGE 142] total=10 new=10 (seen=1436)\n",
            "[PROGRESS] saved=1440\n",
            "[PAGE 143] total=10 new=10 (seen=1446)\n",
            "[PROGRESS] saved=1450\n",
            "[CHECKPOINT] 1450 articles stored…\n",
            "[PAGE 144] total=10 new=10 (seen=1456)\n",
            "[PROGRESS] saved=1460\n",
            "[PAGE 145] total=10 new=10 (seen=1466)\n",
            "[PROGRESS] saved=1470\n",
            "[PAGE 146] total=10 new=10 (seen=1476)\n",
            "[PROGRESS] saved=1480\n",
            "[PAGE 147] total=10 new=10 (seen=1486)\n",
            "[PROGRESS] saved=1490\n",
            "[PAGE 148] total=10 new=10 (seen=1496)\n",
            "[PROGRESS] saved=1500\n",
            "[CHECKPOINT] 1500 articles stored…\n",
            "[PAGE 149] total=10 new=10 (seen=1506)\n",
            "[PROGRESS] saved=1510\n",
            "[PAGE 150] total=10 new=10 (seen=1516)\n",
            "[PROGRESS] saved=1520\n",
            "[PAGE 151] total=10 new=10 (seen=1526)\n",
            "[PROGRESS] saved=1530\n",
            "[PAGE 152] total=10 new=10 (seen=1536)\n",
            "[PROGRESS] saved=1540\n",
            "[PAGE 153] total=10 new=10 (seen=1546)\n",
            "[PROGRESS] saved=1550\n",
            "[CHECKPOINT] 1550 articles stored…\n",
            "[PAGE 154] total=10 new=10 (seen=1556)\n",
            "[PROGRESS] saved=1560\n",
            "[PAGE 155] total=10 new=9 (seen=1566)\n",
            "[PROGRESS] saved=1570\n",
            "[PAGE 156] total=10 new=10 (seen=1575)\n",
            "[PROGRESS] saved=1580\n",
            "[PAGE 157] total=10 new=10 (seen=1585)\n",
            "[PROGRESS] saved=1590\n",
            "[PAGE 158] total=10 new=10 (seen=1595)\n",
            "[PROGRESS] saved=1600\n",
            "[CHECKPOINT] 1600 articles stored…\n",
            "[PAGE 159] total=10 new=10 (seen=1605)\n",
            "[PROGRESS] saved=1610\n",
            "[PAGE 160] total=10 new=10 (seen=1615)\n",
            "[PROGRESS] saved=1620\n",
            "[PAGE 161] total=10 new=10 (seen=1625)\n",
            "[PROGRESS] saved=1630\n",
            "[PAGE 162] total=10 new=10 (seen=1635)\n",
            "[PROGRESS] saved=1640\n",
            "[PAGE 163] total=10 new=10 (seen=1645)\n",
            "[PROGRESS] saved=1650\n",
            "[CHECKPOINT] 1650 articles stored…\n",
            "[PAGE 164] total=10 new=10 (seen=1655)\n",
            "[PROGRESS] saved=1660\n",
            "[PAGE 165] total=10 new=10 (seen=1665)\n",
            "[PROGRESS] saved=1670\n",
            "[PAGE 166] total=10 new=10 (seen=1675)\n",
            "[PROGRESS] saved=1680\n",
            "[PAGE 167] total=10 new=10 (seen=1685)\n",
            "[PROGRESS] saved=1690\n",
            "[PAGE 168] total=10 new=10 (seen=1695)\n",
            "[PROGRESS] saved=1700\n",
            "[CHECKPOINT] 1700 articles stored…\n",
            "[PAGE 169] total=10 new=10 (seen=1705)\n",
            "[PROGRESS] saved=1710\n",
            "[PAGE 170] total=10 new=10 (seen=1715)\n",
            "[PROGRESS] saved=1720\n",
            "[PAGE 171] total=10 new=10 (seen=1725)\n",
            "[PROGRESS] saved=1730\n",
            "[PAGE 172] total=10 new=10 (seen=1735)\n",
            "[PROGRESS] saved=1740\n",
            "[PAGE 173] total=10 new=10 (seen=1745)\n",
            "[PROGRESS] saved=1750\n",
            "[CHECKPOINT] 1750 articles stored…\n",
            "[PAGE 174] total=10 new=10 (seen=1755)\n",
            "[PROGRESS] saved=1760\n",
            "[PAGE 175] total=10 new=10 (seen=1765)\n",
            "[PROGRESS] saved=1770\n",
            "[PAGE 176] total=10 new=10 (seen=1775)\n",
            "[PROGRESS] saved=1780\n",
            "[PAGE 177] total=10 new=10 (seen=1785)\n",
            "[PROGRESS] saved=1790\n",
            "[PAGE 178] total=10 new=10 (seen=1795)\n",
            "[PROGRESS] saved=1800\n",
            "[CHECKPOINT] 1800 articles stored…\n",
            "[PAGE 179] total=10 new=10 (seen=1805)\n",
            "[WARN] https://boursenews.ma/article/marches/ocp-prepare-une-sortie-l-international – attempt 1 failed, retry in 1.4s\n",
            "[PROGRESS] saved=1810\n",
            "[PAGE 180] total=10 new=10 (seen=1815)\n",
            "[PROGRESS] saved=1820\n",
            "[PAGE 181] total=10 new=10 (seen=1825)\n",
            "[PROGRESS] saved=1830\n",
            "[PAGE 182] total=10 new=10 (seen=1835)\n",
            "[PROGRESS] saved=1840\n",
            "[PAGE 183] total=10 new=10 (seen=1845)\n",
            "[PROGRESS] saved=1850\n",
            "[CHECKPOINT] 1850 articles stored…\n",
            "[PAGE 184] total=10 new=10 (seen=1855)\n",
            "[PROGRESS] saved=1860\n",
            "[PAGE 185] total=10 new=10 (seen=1865)\n",
            "[PROGRESS] saved=1870\n",
            "[PAGE 186] total=10 new=10 (seen=1875)\n",
            "[PROGRESS] saved=1880\n",
            "[PAGE 187] total=10 new=10 (seen=1885)\n",
            "[PROGRESS] saved=1890\n",
            "[PAGE 188] total=10 new=10 (seen=1895)\n",
            "[PROGRESS] saved=1900\n",
            "[CHECKPOINT] 1900 articles stored…\n",
            "[PAGE 189] total=10 new=10 (seen=1905)\n",
            "[PROGRESS] saved=1910\n",
            "[PAGE 190] total=10 new=10 (seen=1915)\n",
            "[PROGRESS] saved=1920\n",
            "[PAGE 191] total=10 new=10 (seen=1925)\n",
            "[PROGRESS] saved=1930\n",
            "[PAGE 192] total=10 new=10 (seen=1935)\n",
            "[WARN] https://boursenews.ma/article/marches/groupe-bcp-resultats-2023-en-forte-hausse – attempt 1 failed, retry in 1.4s\n",
            "[PROGRESS] saved=1940\n",
            "[PAGE 193] total=10 new=10 (seen=1945)\n",
            "[PROGRESS] saved=1950\n",
            "[CHECKPOINT] 1950 articles stored…\n",
            "[PAGE 194] total=10 new=10 (seen=1955)\n",
            "[PROGRESS] saved=1960\n",
            "[PAGE 195] total=10 new=10 (seen=1965)\n",
            "[PROGRESS] saved=1970\n",
            "[PAGE 196] total=10 new=10 (seen=1975)\n",
            "[PROGRESS] saved=1980\n",
            "[PAGE 197] total=10 new=10 (seen=1985)\n",
            "[PROGRESS] saved=1990\n",
            "[PAGE 198] total=10 new=10 (seen=1995)\n",
            "[PROGRESS] saved=2000\n",
            "[CHECKPOINT] 2000 articles stored…\n",
            "[PAGE 199] total=10 new=10 (seen=2005)\n",
            "[PROGRESS] saved=2010\n",
            "[PAGE 200] total=10 new=10 (seen=2015)\n",
            "[PROGRESS] saved=2020\n",
            "[PAGE 201] total=10 new=10 (seen=2025)\n",
            "[PROGRESS] saved=2030\n",
            "[PAGE 202] total=10 new=10 (seen=2035)\n",
            "[PROGRESS] saved=2040\n",
            "[PAGE 203] total=10 new=10 (seen=2045)\n",
            "[PROGRESS] saved=2050\n",
            "[CHECKPOINT] 2050 articles stored…\n",
            "[PAGE 204] total=10 new=10 (seen=2055)\n",
            "[PROGRESS] saved=2060\n",
            "[PAGE 205] total=10 new=10 (seen=2065)\n",
            "[PROGRESS] saved=2070\n",
            "[PAGE 206] total=10 new=10 (seen=2075)\n",
            "[PROGRESS] saved=2080\n",
            "[PAGE 207] total=10 new=10 (seen=2085)\n",
            "[PROGRESS] saved=2090\n",
            "[PAGE 208] total=10 new=10 (seen=2095)\n",
            "[PROGRESS] saved=2100\n",
            "[CHECKPOINT] 2100 articles stored…\n",
            "[PAGE 209] total=10 new=10 (seen=2105)\n",
            "[PROGRESS] saved=2110\n",
            "[PAGE 210] total=10 new=10 (seen=2115)\n",
            "[PROGRESS] saved=2120\n",
            "[PAGE 211] total=10 new=10 (seen=2125)\n",
            "[PROGRESS] saved=2130\n",
            "[PAGE 212] total=10 new=10 (seen=2135)\n",
            "[PROGRESS] saved=2140\n",
            "[PAGE 213] total=10 new=10 (seen=2145)\n",
            "[PROGRESS] saved=2150\n",
            "[CHECKPOINT] 2150 articles stored…\n",
            "[PAGE 214] total=10 new=10 (seen=2155)\n",
            "[PROGRESS] saved=2160\n",
            "[PAGE 215] total=10 new=10 (seen=2165)\n",
            "[WARN] https://boursenews.ma/article/marches/tamwilcom-conference-reseau-euro-med – attempt 1 failed, retry in 1.2s\n",
            "[PROGRESS] saved=2170\n",
            "[PAGE 216] total=10 new=10 (seen=2175)\n",
            "[PROGRESS] saved=2180\n",
            "[PAGE 217] total=10 new=10 (seen=2185)\n",
            "[PROGRESS] saved=2190\n",
            "[PAGE 218] total=10 new=10 (seen=2195)\n",
            "[PROGRESS] saved=2200\n",
            "[CHECKPOINT] 2200 articles stored…\n",
            "[PAGE 219] total=10 new=10 (seen=2205)\n",
            "[PROGRESS] saved=2210\n",
            "[PAGE 220] total=10 new=10 (seen=2215)\n",
            "[PROGRESS] saved=2220\n",
            "[PAGE 221] total=10 new=10 (seen=2225)\n",
            "[WARN] https://boursenews.ma/article/marches/jouahri-meilleur-banquier-central – attempt 1 failed, retry in 1.2s\n",
            "[PROGRESS] saved=2230\n",
            "[PAGE 222] total=10 new=10 (seen=2235)\n",
            "[PROGRESS] saved=2240\n",
            "[PAGE 223] total=10 new=10 (seen=2245)\n",
            "[PROGRESS] saved=2250\n",
            "[CHECKPOINT] 2250 articles stored…\n",
            "[PAGE 224] total=10 new=10 (seen=2255)\n",
            "[PROGRESS] saved=2260\n",
            "[PAGE 225] total=10 new=10 (seen=2265)\n",
            "[PROGRESS] saved=2270\n",
            "[PAGE 226] total=10 new=10 (seen=2275)\n",
            "[PROGRESS] saved=2280\n",
            "[PAGE 227] total=10 new=10 (seen=2285)\n",
            "[PROGRESS] saved=2290\n",
            "[PAGE 228] total=10 new=10 (seen=2295)\n"
          ]
        }
      ]
    }
  ]
}